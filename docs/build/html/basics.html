
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Basics &#8212; nnsight 0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Searching for Induction Heads" href="induction.html" />
    <link rel="prev" title="Tutorials" href="tutorials.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="basics">
<h1>Basics<a class="headerlink" href="#basics" title="Permalink to this heading">¶</a></h1>
<p>Content</p>
<ul class="simple">
<li><p><a class="reference internal" href="#loading-models"><span class="std std-ref">Loading Models</span></a></p></li>
<li><p><a class="reference internal" href="#accessing-activations"><span class="std std-ref">Accessing Activations</span></a></p></li>
<li><p><a class="reference internal" href="#intervening"><span class="std std-ref">Intervening on Activations</span></a></p></li>
</ul>
<section id="loading-models">
<span id="id1"></span><h2>Loading Models<a class="headerlink" href="#loading-models" title="Permalink to this heading">¶</a></h2>
<p>The Engine API allows you to access open source LLMs by referencing a Hugging Face repo ID.
For this demo we’ll look at <a class="reference external" href="https://huggingface.co/gpt2">GPT-2 Small</a>, an 80M parameter model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Declare the model and load onto device</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LanguageModel</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">,</span><span class="n">device_map</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>To try the model out, let’s generate some text!</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">prompt</span> <span class="o">=</span> <span class="s1">&#39;The Eiffel Tower is in the city of&#39;</span>

<span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">as</span> <span class="n">generator</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="k">as</span> <span class="n">invoker</span><span class="p">:</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">generator</span><span class="o">.</span><span class="n">output</span>

<span class="nb">print</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">output</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="p">[</span><span class="s1">&#39;The Eiffel Tower is in the city of Paris&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>Let’s go over this piece by piece.</p>
<p><strong>(1) We create a generation context block</strong> by calling <code class="docutils literal notranslate"><span class="pre">.generate(...)</span></code> on the model object. This denotes that we wish to generate tokens given some prompts.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">as</span> <span class="n">generator</span><span class="p">:</span>
</pre></div>
</div>
<p>Calling <code class="docutils literal notranslate"><span class="pre">.generate(...)</span></code> does not actually initialize or run the model. Only after the block is exited is the model actually loaded and run. All operations in the block are proxies which create a graph of operations to be carried out later.</p>
<p><strong>(2) Within the generation context,</strong> we create invocation contexts to specify the actual prompts we want to run.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="k">as</span> <span class="n">invoker</span><span class="p">:</span>
</pre></div>
</div>
<p><strong>(3) Within an invoke context</strong>, all operations/interventions will be applied to the processing of the prompt. Models can be run on a variety of input formats. See model inputs for more.</p>
<p>Finally, we can access raw tensors and activations at any point in the model. <em>But what can we do with these activations?</em></p>
</section>
<section id="accessing-activations">
<span id="id2"></span><h2>Accessing Activations<a class="headerlink" href="#accessing-activations" title="Permalink to this heading">¶</a></h2>
<p>The first basic operation when doing mechanistic interpretability is to break open the black box
and look at all of the internal activations of the model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">gpt2_text</span> <span class="o">=</span> <span class="s2">&quot;Natural language processing tasks, such as...&quot;</span>
<span class="n">gpt2_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">gpt2_text</span><span class="p">)</span>

<span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">as</span> <span class="n">generator</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">gpt2_tokens</span><span class="p">)</span> <span class="k">as</span> <span class="n">invoker</span><span class="p">:</span>
<span class="hll">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
</span></pre></div>
</div>
<p>Lets focus on the highlighted line.</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">model.transformer.h[-1]</span></code> accesses a module in the computation graph, specifically the last transformer layer.</p>
<p><code class="docutils literal notranslate"><span class="pre">.output</span></code> returns a proxy for the output of this module. In other words, when we get to the output of this module during inference, grab it and perform any operations we define on it. The outputs become two operational proxies, one for getting the 0th index of the output, and one for saving the output. We take the 0th index because the output of gpt2 transformer layers are a <em>tuple</em> where the first index is the actual hidden states and the last two are from attention.</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">.shape</span></code> can be called on any proxy to get what shape the value will eventually be.</p>
<p><code class="docutils literal notranslate"><span class="pre">.input</span></code> similarly returns a proxy for the inputs to this module.</p>
</div></blockquote>
<p><code class="docutils literal notranslate"><span class="pre">.save()</span></code> informs the computation graph to clone the value of a proxy, allowing us to access the value of a proxy after generation. During processing of the intervention computational graph we are building, when the value of a proxy is no longer ever needed, its value is dereferenced and destroyed.</p>
</div></blockquote>
<p>After exiting the generator context, the model is ran with the specified arguments and intervention graph. <code class="docutils literal notranslate"><span class="pre">generator.output</span></code> is populated with the actual output and <code class="docutils literal notranslate"><span class="pre">hidden_states.value</span></code> will contain the value.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">generator</span><span class="o">.</span><span class="n">output</span>
<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">value</span>

<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
</pre></div>
</div>
<p>Should return:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">([[</span><span class="mi">35364</span><span class="p">,</span>  <span class="mi">3303</span><span class="p">,</span>  <span class="mi">7587</span><span class="p">,</span>  <span class="mi">8861</span><span class="p">,</span>    <span class="mi">11</span><span class="p">,</span>   <span class="mi">884</span><span class="p">,</span>   <span class="mi">355</span><span class="p">,</span>  <span class="mi">1808</span><span class="p">,</span> <span class="mi">18877</span><span class="p">,</span>    <span class="mi">11</span><span class="p">,</span>
      <span class="mi">4572</span><span class="p">,</span> <span class="mi">11059</span><span class="p">,</span>    <span class="mi">11</span><span class="p">,</span>  <span class="mi">3555</span><span class="p">,</span> <span class="mi">35915</span><span class="p">,</span>    <span class="mi">11</span><span class="p">,</span>   <span class="mi">290</span><span class="p">,</span> <span class="mi">15676</span><span class="p">,</span>  <span class="mi">1634</span><span class="p">,</span>    <span class="mi">11</span><span class="p">,</span>
       <span class="mi">389</span><span class="p">,</span>  <span class="mi">6032</span><span class="p">,</span> <span class="mi">10448</span><span class="p">,</span>   <span class="mi">351</span><span class="p">,</span> <span class="mi">28679</span><span class="p">,</span>  <span class="mi">4673</span><span class="p">,</span>   <span class="mi">319</span><span class="p">,</span>  <span class="mi">8861</span><span class="p">,</span>   <span class="mi">431</span><span class="p">,</span>  <span class="mi">7790</span><span class="p">,</span>
     <span class="mi">40522</span><span class="p">,</span>    <span class="mi">13</span><span class="p">,</span>  <span class="mi">2102</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[[</span> <span class="o">-</span><span class="mf">0.2059</span><span class="p">,</span>   <span class="mf">0.1688</span><span class="p">,</span>  <span class="o">-</span><span class="mf">2.0503</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="o">-</span><span class="mf">0.3703</span><span class="p">,</span>  <span class="o">-</span><span class="mf">0.2015</span><span class="p">,</span>  <span class="o">-</span><span class="mf">1.6594</span><span class="p">],</span>
        <span class="p">[</span> <span class="o">-</span><span class="mf">3.9412</span><span class="p">,</span>  <span class="o">-</span><span class="mf">0.2137</span><span class="p">,</span>  <span class="o">-</span><span class="mf">8.5667</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>   <span class="mf">6.3562</span><span class="p">,</span>   <span class="mf">4.1276</span><span class="p">,</span>   <span class="mf">3.6006</span><span class="p">],</span>
        <span class="p">[</span> <span class="o">-</span><span class="mf">2.0798</span><span class="p">,</span>  <span class="o">-</span><span class="mf">1.5781</span><span class="p">,</span>  <span class="o">-</span><span class="mf">6.1944</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>   <span class="mf">4.8023</span><span class="p">,</span>   <span class="mf">5.6864</span><span class="p">,</span>  <span class="o">-</span><span class="mf">2.6289</span><span class="p">],</span>
        <span class="o">...</span><span class="p">,</span>
        <span class="p">[</span> <span class="o">-</span><span class="mf">2.1180</span><span class="p">,</span>  <span class="o">-</span><span class="mf">6.4320</span><span class="p">,</span> <span class="o">-</span><span class="mf">20.7147</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>   <span class="mf">8.7145</span><span class="p">,</span>   <span class="mf">2.3738</span><span class="p">,</span>   <span class="mf">3.4004</span><span class="p">],</span>
        <span class="p">[</span> <span class="o">-</span><span class="mf">1.1358</span><span class="p">,</span>  <span class="o">-</span><span class="mf">3.9569</span><span class="p">,</span> <span class="o">-</span><span class="mf">20.3060</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>   <span class="mf">7.1600</span><span class="p">,</span>   <span class="mf">1.6868</span><span class="p">,</span>   <span class="mf">0.9850</span><span class="p">],</span>
        <span class="p">[</span> <span class="o">-</span><span class="mf">1.7206</span><span class="p">,</span>  <span class="o">-</span><span class="mf">4.7800</span><span class="p">,</span>  <span class="o">-</span><span class="mf">1.1185</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>   <span class="mf">3.1680</span><span class="p">,</span>   <span class="mf">3.7024</span><span class="p">,</span>   <span class="mf">0.2865</span><span class="p">]]],</span>
    <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="intervening-on-activations">
<span id="intervening"></span><h2>Intervening on Activations<a class="headerlink" href="#intervening-on-activations" title="Permalink to this heading">¶</a></h2>
<p>One of the great things about interpreting neural networks is that we have <em>full control</em> over our system. From a computational perspective, we know exactly what operations are going on inside (even if we don’t know what they mean!). And we can make precise, surgical edits and see how the model’s behaviour and other internals change. This is an extremely powerful tool, because it can let us set up careful counterfactuals and causal intervention to easily understand model behaviour.</p>
<p>Accordingly, being able to do this is a pretty core operation, and this is one of the main things the Engine API supports! The key features here are <strong>operation</strong> and <strong>setting</strong>. Within an invoke context, most basic operations and torch operations work on proxies and are added to the computation graph. We can also use the assignment <code class="docutils literal notranslate"><span class="pre">=</span></code> operator to edit and intervene on the flow of information.</p>
<p>As a basic example, let’s <a class="reference external" href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=fh-HJyz1CgUVrXuoiban6bYx">ablate</a> head 7 in layer 0 on the text above.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">layer_to_ablate</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">head_index_to_ablate</span> <span class="o">=</span> <span class="mi">7</span>

<span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">as</span> <span class="n">generator</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">gpt2_tokens</span><span class="p">)</span> <span class="k">as</span> <span class="n">invoker</span><span class="p">:</span>
        <span class="n">normal_lm_head</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">gpt2_tokens</span><span class="p">)</span> <span class="k">as</span> <span class="n">invoker</span><span class="p">:</span>
        <span class="n">attention_pattern</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">layer_to_ablate</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">output</span>
        <span class="n">attention_pattern</span><span class="p">[:,</span><span class="n">head_index_to_ablate</span><span class="p">,:,:]</span> <span class="o">=</span> <span class="mf">0.</span>
        <span class="n">ablated_lm_head</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="n">normal_lm_head</span> <span class="o">=</span> <span class="n">normal_lm_head</span><span class="o">.</span><span class="n">value</span>
<span class="n">ablated_lm_head</span> <span class="o">=</span> <span class="n">ablated_lm_head</span><span class="o">.</span><span class="n">value</span>
</pre></div>
</div>
<p>As a result of ablating the head, we see a noticable change in loss.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tensor_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">gpt2_tokens</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">normal_lm_head</span><span class="p">,</span> <span class="n">tensor_tokens</span><span class="p">,</span> <span class="n">shift</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tensor</span><span class="p">(</span><span class="mf">4.0187</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">abalated_lm_head</span><span class="p">,</span> <span class="n">tensor_tokens</span><span class="p">,</span> <span class="n">shift</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tensor</span><span class="p">(</span><span class="mf">4.2913</span><span class="p">)</span>
</pre></div>
</div>
<p>Now that we’ve covered the basic tools of the Engine API, how can we use these tools to understand broader model behavior?</p>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h3><a href="index.html">Table of Contents</a></h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="start.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="generator.html">Generator</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="tutorials.html">Tutorials</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="induction.html">Searching for Induction Heads</a></li>
<li class="toctree-l2"><a class="reference internal" href="patching.html">Patching</a></li>
</ul>
</li>
</ul>

<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, Jaden Fiotto-Kaufman.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 5.3.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="_sources/basics.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>