

<!DOCTYPE html>


<html lang="en" data-theme="light">

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>NDIF Main Demo Notebook &#8212; nnsight 0.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "light";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/nbsphinx-code-cells.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/design-tabs.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'tutorials/nnsight_exercises';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Main Demo" href="main_demo.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="light">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Your text here..."
         aria-label="Your text here..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  <div class="navbar-header-items__start">
    
      <div class="navbar-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    <p class="title logo__title">nnsight</p>
  
</a></div>
    
  </div>
  
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../start.html">
                        Getting Started
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../documentation.html">
                        Documentation
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../tutorials.html">
                        Tutorials
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="#">
                        NDIF Main Demo Notebook
                      </a>
                    </li>
                
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
        </div>
      
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
    </div>
  

  
    <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fa-solid fa-outdent"></span>
    </label>
  
</div>

    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../start.html">
                        Getting Started
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../documentation.html">
                        Documentation
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../tutorials.html">
                        Tutorials
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="#">
                        NDIF Main Demo Notebook
                      </a>
                    </li>
                
  </ul>
</nav></div>
        
      </div>
    
    
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumbs">
  <ul class="bd-breadcrumbs" role="navigation" aria-label="Breadcrumb">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page">NDIF Main Demo Notebook</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="NDIF-Main-Demo-Notebook">
<h1>NDIF Main Demo Notebook<a class="headerlink" href="#NDIF-Main-Demo-Notebook" title="Permalink to this heading">#</a></h1>
<p>These exercises are written in the style of the other ARENA exercises - see GDrive directory containing other Colab notebooks <a class="reference external" href="https://drive.google.com/drive/folders/1UTg3Xsmf77IYFFfj17dagJIsMKwmPsAi">here</a>.</p>
<p><img alt="fb64fe000d4641b795e557d431ddfde6" class="no-scaled-link" src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/fv_header.png" style="width: 350px;" /></p>
<p>Note - I’ve not run code in this Colab yet beyond the “Setup” section, because it’ll cause OOM errors unless you’ve got Pro+ or smth.</p>
<p>However, I’ve left all the output displayed from when I ran the code in my <code class="docutils literal notranslate"><span class="pre">.ipynb</span></code> file on a virtual machine (where it did work). So you can see what it looks like when the code actually runs!</p>
<section id="Content-&amp;-Learning-Objectives">
<h2>Content &amp; Learning Objectives<a class="headerlink" href="#Content-&-Learning-Objectives" title="Permalink to this heading">#</a></h2>
<section id="1️⃣-Introduction-to-nnsight">
<h3>1️⃣ Introduction to <code class="docutils literal notranslate"><span class="pre">nnsight</span></code><a class="headerlink" href="#1️⃣-Introduction-to-nnsight" title="Permalink to this heading">#</a></h3>
<p>In this section, you’ll learn the basics of how to use the <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> library: running forward passes on your model, and saving the internal states. You’ll also learn some basics of HuggingFace models which translate over into <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> models (e.g. tokenization, and how to work with model output).</p>
</section>
<section id="2️⃣-Task-encoding-hidden-states">
<h3>2️⃣ Task-encoding hidden states<a class="headerlink" href="#2️⃣-Task-encoding-hidden-states" title="Permalink to this heading">#</a></h3>
<p>We begin with the following question, posed by the Function Vectors paper:</p>
<blockquote>
<div><p><em>When a transformer processes an ICL (in-context-learning) prompt with exemplars demonstrating task :math:`T`, do any hidden states encode the task itself?</em></p>
</div></blockquote>
<p>We’ll prove that the answer is yes, by constructing a vector <span class="math notranslate nohighlight">\(h\)</span> from a set of ICL prompts for the <strong>antonym task</strong>, and intervening with our vector to make our model produce antonyms on zero-shot prompts.</p>
<p>This will require you to learn how to perform causal interventions with <code class="docutils literal notranslate"><span class="pre">nnsight</span></code>, not just save activations.</p>
</section>
<section id="3️⃣-Function-Vectors">
<h3>3️⃣ Function Vectors<a class="headerlink" href="#3️⃣-Function-Vectors" title="Permalink to this heading">#</a></h3>
<p>In this section, we’ll replicate the crux of the paper’s results, by identifying a set of attention heads whose outputs have a large effect on the model’s ICL performance, and showing we can patch with these vectors to induce task-solving behaviour on randomly shuffled prompts.</p>
<p>We’ll also learn how to use <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> for multi-token generation, and steer the model’s behaviour. There exist exercises where you can try this out for different tasks, e.g. the Country-Capitals task, where you’ll be able to steer the model to complete prompts like <code class="docutils literal notranslate"><span class="pre">&quot;When</span> <span class="pre">you</span> <span class="pre">think</span> <span class="pre">of</span> <span class="pre">Netherlands,</span> <span class="pre">you</span> <span class="pre">usually</span> <span class="pre">think</span> <span class="pre">of&quot;</span></code> by talking about Amsterdam.</p>
</section>
<section id="4️⃣-Bonus">
<h3>4️⃣ Bonus<a class="headerlink" href="#4️⃣-Bonus" title="Permalink to this heading">#</a></h3>
<p>A few bonus exercises are suggested, to wrap up some of the paper’s other results (e.g. the decoding vocabulary of function vectors, and vector arithmetic with function vectors).</p>
</section>
</section>
</section>
<section id="Setup">
<h1>Setup<a class="headerlink" href="#Setup" title="Permalink to this heading">#</a></h1>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">google.colab</span>
    <span class="n">IN_COLAB</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">except</span><span class="p">:</span>
    <span class="n">IN_COLAB</span> <span class="o">=</span> <span class="kc">False</span>

<span class="k">if</span> <span class="n">IN_COLAB</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">os</span><span class="o">,</span> <span class="nn">sys</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s2">&quot;fn_vectors&quot;</span><span class="p">):</span>
        <span class="o">!</span>wget<span class="w"> </span>https://github.com/callummcdougall/fn_vectors/archive/refs/heads/main.zip
        <span class="o">!</span>unzip<span class="w"> </span>/content/main.zip
        <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s2">&quot;/content/main.zip&quot;</span><span class="p">)</span>
        <span class="n">os</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s2">&quot;fn_vectors-main&quot;</span><span class="p">,</span> <span class="s2">&quot;fn_vectors&quot;</span><span class="p">)</span>
        <span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="s2">&quot;fn_vectors&quot;</span><span class="p">)</span>

        <span class="o">%</span><span class="k">pip</span> install -e ndif-dev
        <span class="o">%</span><span class="k">pip</span> install einops
        <span class="o">%</span><span class="k">pip</span> install jaxtyping
        <span class="o">%</span><span class="k">pip</span> install plotly
        <span class="o">%</span><span class="k">pip</span> install transformer_lens
        <span class="o">%</span><span class="k">pip</span> install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python
        <span class="o">%</span><span class="k">pip</span> install --upgrade pydantic

    <span class="c1"># Clear output</span>
    <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">clear_output</span>
    <span class="n">clear_output</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Imports &amp; installations complete!&quot;</span><span class="p">)</span>

<span class="k">else</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">get_ipython</span>
    <span class="n">ipython</span> <span class="o">=</span> <span class="n">get_ipython</span><span class="p">()</span>
    <span class="n">ipython</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s2">&quot;load_ext&quot;</span><span class="p">,</span> <span class="s2">&quot;autoreload&quot;</span><span class="p">)</span>
    <span class="n">ipython</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s2">&quot;autoreload&quot;</span><span class="p">,</span> <span class="s2">&quot;2&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Imports &amp; installations complete!
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span> <span class="nn">gdown</span>
<span class="kn">import</span> <span class="nn">zipfile</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">clear_output</span>

<span class="c1"># Check file structure is as expected</span>
<span class="n">root</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;/content/fn_vectors&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">resolve</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">root</span><span class="o">.</span><span class="n">exists</span><span class="p">()</span>

<span class="c1"># Add to sys.path, so we can import engine</span>
<span class="n">ndif_root</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">root</span> <span class="o">/</span> <span class="s2">&quot;ndif-dev&quot;</span><span class="p">)</span>
<span class="k">if</span> <span class="n">ndif_root</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="p">:</span> <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ndif_root</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">engine</span>
<span class="kn">from</span> <span class="nn">engine</span> <span class="kn">import</span> <span class="n">LanguageModel</span>
<span class="kn">from</span> <span class="nn">engine.intervention</span> <span class="kn">import</span> <span class="n">InterventionProxy</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span> <span class="k">as</span> <span class="nn">t</span>
<span class="kn">import</span> <span class="nn">einops</span>
<span class="kn">import</span> <span class="nn">circuitsvis</span> <span class="k">as</span> <span class="nn">cv</span>
<span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="nn">px</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">jaxtyping</span> <span class="kn">import</span> <span class="n">Int</span><span class="p">,</span> <span class="n">Float</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">import</span> <span class="nn">webbrowser</span>
<span class="kn">from</span> <span class="nn">rich</span> <span class="kn">import</span> <span class="nb">print</span> <span class="k">as</span> <span class="n">rprint</span>
<span class="kn">from</span> <span class="nn">rich.table</span> <span class="kn">import</span> <span class="n">Table</span>
<span class="kn">import</span> <span class="nn">openai</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">gc</span>
<span class="kn">import</span> <span class="nn">string</span>

<span class="kn">from</span> <span class="nn">plotly_utils</span> <span class="kn">import</span> <span class="n">imshow</span><span class="p">,</span> <span class="n">line</span>
<span class="kn">import</span> <span class="nn">fn_vectors.tests</span> <span class="k">as</span> <span class="nn">tests</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="n">t</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">False</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># gpt2 = LanguageModel(&#39;gpt2&#39;, device_map=device)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LanguageModel</span><span class="p">(</span><span class="s1">&#39;EleutherAI/gpt-j-6b&#39;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span>
</pre></div>
</div>
</div>
</section>
<section id="id1">
<h1>1️⃣ Introduction to <code class="docutils literal notranslate"><span class="pre">nnsight</span></code><a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h1>
<section id="Important-syntax">
<h2>Important syntax<a class="headerlink" href="#Important-syntax" title="Permalink to this heading">#</a></h2>
<p>Here, we’ll discuss some important syntax for interacting with <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> models. Since these models are extensions of HuggingFace models, some of this information (e.g. tokenization) applies to plain HuggingFace models as well as <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> models, and some of it (e.g. forward passes) is specific to <code class="docutils literal notranslate"><span class="pre">nnsight</span></code>, i.e. it would work differently if you just had a standard HuggingFace model. Before each section, we’ll indicate which is which.</p>
<section id="Model-config">
<h3>Model config<a class="headerlink" href="#Model-config" title="Permalink to this heading">#</a></h3>
<p><em>This applies to HuggingFace and ``nnsight`` models.</em></p>
<p>Each model comes with a <code class="docutils literal notranslate"><span class="pre">model.config</span></code>, which contains lots of useful information about the model (e.g. number of heads and layers, size of hidden layers, etc.). You can access this with <code class="docutils literal notranslate"><span class="pre">model.config</span></code>. Run the code below to see this in action, and to define some useful variables for later.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">N_HEADS</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_head</span>
<span class="n">N_LAYERS</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_layer</span>
<span class="n">D_MODEL</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_embd</span>
<span class="n">D_HEAD</span> <span class="o">=</span> <span class="n">D_MODEL</span> <span class="o">//</span> <span class="n">N_HEADS</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of heads: </span><span class="si">{</span><span class="n">N_HEADS</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of layers: </span><span class="si">{</span><span class="n">N_LAYERS</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model dimension: </span><span class="si">{</span><span class="n">D_MODEL</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Head dimension: </span><span class="si">{</span><span class="n">D_HEAD</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Number of heads: 16
Number of layers: 28
Model dimension: 4096
Head dimension: 256

GPTJConfig {
  &#34;_name_or_path&#34;: &#34;EleutherAI/gpt-j-6b&#34;,
  &#34;activation_function&#34;: &#34;gelu_new&#34;,
  &#34;architectures&#34;: [
    &#34;GPTJForCausalLM&#34;
  ],
  &#34;attn_pdrop&#34;: 0.0,
  &#34;bos_token_id&#34;: 50256,
  &#34;embd_pdrop&#34;: 0.0,
  &#34;eos_token_id&#34;: 50256,
  &#34;gradient_checkpointing&#34;: false,
  &#34;initializer_range&#34;: 0.02,
  &#34;layer_norm_epsilon&#34;: 1e-05,
  &#34;model_type&#34;: &#34;gptj&#34;,
  &#34;n_embd&#34;: 4096,
  &#34;n_head&#34;: 16,
  &#34;n_inner&#34;: null,
  &#34;n_layer&#34;: 28,
  &#34;n_positions&#34;: 2048,
  &#34;resid_pdrop&#34;: 0.0,
  &#34;rotary&#34;: true,
  &#34;rotary_dim&#34;: 64,
  &#34;scale_attn_weights&#34;: true,
  &#34;summary_activation&#34;: null,
  &#34;summary_first_dropout&#34;: 0.1,
  &#34;summary_proj_to_labels&#34;: true,
  &#34;summary_type&#34;: &#34;cls_index&#34;,
  &#34;summary_use_proj&#34;: true,
  &#34;task_specific_params&#34;: {
    &#34;text-generation&#34;: {
      &#34;do_sample&#34;: true,
      &#34;max_length&#34;: 50,
      &#34;temperature&#34;: 1.0
    }
  },
  &#34;tie_word_embeddings&#34;: false,
  &#34;tokenizer_class&#34;: &#34;GPT2Tokenizer&#34;,
  &#34;transformers_version&#34;: &#34;4.33.3&#34;,
  &#34;use_cache&#34;: true,
  &#34;vocab_size&#34;: 50400
}

</pre></div></div>
</div>
</section>
<section id="Tokenizers">
<h3>Tokenizers<a class="headerlink" href="#Tokenizers" title="Permalink to this heading">#</a></h3>
<p><em>This applies to HuggingFace and ``nnsight`` models.</em></p>
<p>A model comes with a tokenizer, accessable with <code class="docutils literal notranslate"><span class="pre">model.tokenizer</span></code> (just like TransformerLens). Unlike TransformerLens, we won’t be using utility functions like <code class="docutils literal notranslate"><span class="pre">model.to_str_toks</span></code>, instead we’ll be using the tokenizer directly. Some important functions for today’s exercises are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tokenizer</span></code> (i.e. just calling it on some input)</p>
<ul>
<li><p>This takes in a string (or list of strings) and returns the tokenized version.</p></li>
<li><p>It will return a dictionary, always containing <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> (i.e. the actual tokens) but also other things which are specific to the transformer model (e.g. <code class="docutils literal notranslate"><span class="pre">attention_mask</span></code> - see dropdown).</p></li>
<li><p>Other useful arguments for this function:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">return_tensors</span></code> - if this is <code class="docutils literal notranslate"><span class="pre">&quot;pt&quot;</span></code>, you’ll get results returned as PyTorch tensors, rather than lists (which is the default).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">padding</span></code> - if True (default is False), the tokenizer can accept sequences of variable length. The shorter sequences get padded at the beginning (see dropdown below for more).</p></li>
</ul>
</li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">tokenizer.decode</span></code></p>
<ul>
<li><p>This takes in tokens, and returns the decoded string.</p></li>
<li><p>If the input is an integer, it returns the corresponding string. If the input is a list / 1D array of integers, it returns all those strings concatenated (which can sometimes not be what you want).</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">tokenizer.batch_decode</span></code></p>
<ul>
<li><p>Equivalent to <code class="docutils literal notranslate"><span class="pre">tokenizer.decode</span></code>, but it doesn’t concatenate.</p></li>
<li><p>If the input is a list / 1D integer array, it returns a list of strings. If the input is 2D, it will concatenate within each list.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">tokenizer.tokenize</span></code></p>
<ul>
<li><p>Takes in a string, and returns a list of strings.</p></li>
</ul>
</li>
</ul>
<p>Run the code below to see some examples of these functions in action.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calling tokenizer returns a dictionary, containing input ids &amp; other data.</span>
<span class="c1"># If returned as a tensor, then by default it will have a batch dimension.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;This must be Thursday&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">))</span>

<span class="c1"># Decoding a list of integers, into a concatenated string.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="mi">40</span><span class="p">,</span> <span class="mi">1239</span><span class="p">,</span> <span class="mi">714</span><span class="p">,</span> <span class="mi">651</span><span class="p">,</span> <span class="mi">262</span><span class="p">,</span> <span class="mi">8181</span><span class="p">,</span> <span class="mi">286</span><span class="p">,</span> <span class="mi">48971</span><span class="p">,</span> <span class="mi">12545</span><span class="p">,</span> <span class="mi">13</span><span class="p">]))</span>

<span class="c1"># Using batch decode, on both 1D and 2D input.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">([</span><span class="mi">4711</span><span class="p">,</span> <span class="mi">2456</span><span class="p">,</span> <span class="mi">481</span><span class="p">,</span> <span class="mi">307</span><span class="p">,</span> <span class="mi">6626</span><span class="p">,</span> <span class="mi">510</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">([[</span><span class="mi">1212</span><span class="p">,</span> <span class="mi">6827</span><span class="p">,</span> <span class="mi">481</span><span class="p">,</span> <span class="mi">307</span><span class="p">,</span> <span class="mi">1978</span><span class="p">],</span> <span class="p">[</span><span class="mi">2396</span><span class="p">,</span> <span class="mi">481</span><span class="p">,</span> <span class="mi">428</span><span class="p">,</span> <span class="mi">530</span><span class="p">]]))</span>

<span class="c1"># Split sentence into tokens (note we see the special Ġ character in place of prepended spaces).</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s2">&quot;This sentence will be tokenized&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{&#39;input_ids&#39;: tensor([[1212, 1276,  307, 3635]]), &#39;attention_mask&#39;: tensor([[1, 1, 1, 1]])}
I never could get the hang of Thursdays.
[&#39;These&#39;, &#39; words&#39;, &#39; will&#39;, &#39; be&#39;, &#39; split&#39;, &#39; up&#39;]
[&#39;This sentence will be together&#39;, &#39;So will this one&#39;]
[&#39;This&#39;, &#39;Ġsentence&#39;, &#39;Ġwill&#39;, &#39;Ġbe&#39;, &#39;Ġtoken&#39;, &#39;ized&#39;]
</pre></div></div>
</div>
<details><summary><p>Note on attention_mask (optional)</p>
</summary><p><code class="docutils literal notranslate"><span class="pre">attention_mask</span></code>, which is a series of 1s and 0s. We mask attention at all 0-positions (i.e. we don’t allow these tokens to be attended to). This is useful when you have to do padding. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Hello world&quot;</span><span class="p">,</span> <span class="s2">&quot;Hello&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>will return:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>{
    &#39;attention_mask&#39;: tensor([[1, 1], [0, 1]]),
    &#39;input_ids&#39;: tensor([[15496,   995], [50256, 15496]])
}
</pre></div>
</div>
<p>We can see how the shorter sequence has been padded at the beginning, and attention to this token will be masked.</p>
</details></section>
<section id="Model-outputs">
<h3>Model outputs<a class="headerlink" href="#Model-outputs" title="Permalink to this heading">#</a></h3>
<p><em>This applies to HuggingFace and ``nnsight`` models.</em></p>
<p>If you’ve worked with TransformerLens, then you’ll be used to thinking of logits as the default output of a model, when you run a forward pass on that model.</p>
<p>HuggingFace models are a bit different. The standard way to get output from them is using the <code class="docutils literal notranslate"><span class="pre">model.generate</span></code> method. This method takes in a dictionary of inputs (which you can get from the tokenizer), and returns an object which contains a bunch of different things: the actual tokens generated by the model, plus maybe a few other things depending on what arguments you passed to <code class="docutils literal notranslate"><span class="pre">generate</span></code> (e.g. this might include logits, or hidden states).</p>
<p>The <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> models we’ll be using here are based on HuggingFace models, and we’ll also be using <code class="docutils literal notranslate"><span class="pre">model.generate</span></code> which takes basically the same arguments, and produces an output object that contains the same kind of information. However, the exact way we use this method is quite different for <code class="docutils literal notranslate"><span class="pre">nnsight</span></code>…</p>
</section>
<section id="Running-the-model">
<h3>Running the model<a class="headerlink" href="#Running-the-model" title="Permalink to this heading">#</a></h3>
<p><em>This only applies to ``nnsight`` models.</em></p>
<p>Rather than just calling <code class="docutils literal notranslate"><span class="pre">model.generate</span></code>, we use a <strong>context manager</strong> to run the model. This is useful because we can access &amp; do things with the internal state of the model, in the middle of the forward pass. Using this context manager is like setting up a set of detailed instructions for how the forward pass will work, and only when you exit the context manager are the instructions actually sent off &amp; executed.</p>
<p>Below is the simplest example of code to run the model (and also access the internal states of the model). Run it and look at the output, then read the explanation below.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>

<span class="c1"># Running this code so we don&#39;t get this printout each time we run a fwd pass</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s2">&quot;Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.&quot;</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prompt</span> <span class="o">=</span> <span class="s1">&#39;The Eiffel Tower is in the city of&#39;</span>

<span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">)</span> <span class="k">as</span> <span class="n">generator</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="k">as</span> <span class="n">invoker</span><span class="p">:</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">InterventionProxy</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="c1"># Get output, which is a tensor of token IDs, of shape (1, seq_len+1)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">generator</span><span class="o">.</span><span class="n">output</span>
<span class="nb">print</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">output</span><span class="p">])</span>

<span class="c1"># Get hidden states, which are the value of the residual stream at last layer</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Residual stream shape = &quot;</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[&#39;The Eiffel Tower is in the city of Paris,&#39;]
Residual stream shape =  torch.Size([1, 10, 4096])
</pre></div></div>
</div>
<p>Lets go over this piece by piece.</p>
<p><strong>First, we create a generation context block</strong> by calling <code class="docutils literal notranslate"><span class="pre">.generate(...)</span></code> on the model object. This denotes that we wish to generate tokens given some prompts.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">)</span> <span class="k">as</span> <span class="n">generator</span><span class="p">:</span>
</pre></div>
</div>
<p>Calling <code class="docutils literal notranslate"><span class="pre">.generate(...)</span></code> does not actually initialize or run the model. Only after the <code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">...</span> <span class="pre">as</span> <span class="pre">generator:</span></code> block is exited is the model actually loaded and run. All operations in the block are “proxies” which essentially creates a graph of operations we wish to carry out later.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">max_new_tokens=1</span></code> argument just means we do a single forward pass, rather than autoregressively generate multiple tokens. The <code class="docutils literal notranslate"><span class="pre">pad_token_id</span></code> argument isn’t strictly necessary (this is the default behaviour anyway), it just suppresses a warning message that would otherwise be printed.</p>
<p><strong>Within the generation context,</strong> we create invocation contexts to specify the actual prompts we want to run.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">PROMPT</span><span class="p">)</span> <span class="k">as</span> <span class="n">invoker</span><span class="p">:</span>
</pre></div>
</div>
<p><strong>Within an invoke context</strong>, all operations/interventions will be applied to the processing of the prompt. Models can be run on a variety of input formats: strings, lists of tokens, tensors of tokens, etc.</p>
<p>This is all we actually need to run a forward pass on the model. We could replace the <code class="docutils literal notranslate"><span class="pre">hidden_states</span></code> line with just <code class="docutils literal notranslate"><span class="pre">pass</span></code>, and we’d still be able to access the model output in the same way. But the most interesting part of <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> is the ability to access the model’s internal states (like you’ve probably already done with TransformerLens). Let’s see how this works!</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>hidden_states = model.transformer.h[-1].output[0].save()
</pre></div>
</div>
<p>On this line we’re saying: access the last layer of the transformer <code class="docutils literal notranslate"><span class="pre">model.transformer.h[-1]</span></code>, access this layer’s output <code class="docutils literal notranslate"><span class="pre">.output</span></code> (which is a tuple of tensors), index the first tensor in this tuple <code class="docutils literal notranslate"><span class="pre">.output[0]</span></code>, and save it <code class="docutils literal notranslate"><span class="pre">.save()</span></code>.</p>
<p>Let’s break down this line in a bit more detail:</p>
<section id="model.transformer.h[-1]">
<h4><code class="docutils literal notranslate"><span class="pre">model.transformer.h[-1]</span></code><a class="headerlink" href="#model.transformer.h[-1]" title="Permalink to this heading">#</a></h4>
<p>If you print out the model’s architecture with <code class="docutils literal notranslate"><span class="pre">print(model)</span></code>, you’ll see that it consists of <code class="docutils literal notranslate"><span class="pre">transformer</span></code> and <code class="docutils literal notranslate"><span class="pre">lm_head</span></code> (for “language modelling head”). The <code class="docutils literal notranslate"><span class="pre">transformer</span></code> module is made up of embeddings &amp; dropout, a series of layers (called <code class="docutils literal notranslate"><span class="pre">.h</span></code>, for “hidden states”), and a final layernorm.</p>
<p>When you’re working with different model architectures, it’ll often be necessary to print out the model / visit the source code page, to see exactly how they work and what different modules are named. <a class="reference external" href="https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html">Here</a> is the source code page for GPT-J.</p>
</section>
<section id=".output[0]">
<h4><code class="docutils literal notranslate"><span class="pre">.output[0]</span></code><a class="headerlink" href="#.output[0]" title="Permalink to this heading">#</a></h4>
<p>When you access <code class="docutils literal notranslate"><span class="pre">.output</span></code> of a module within a context manager, you’re returning a <strong>proxy</strong> for the output of this module during inference. Doing operations on it (like indexing it) also return proxies.</p>
<p>Note, modules often have output (and input) stored in tuples. Even if there is only one output and one input, these can be stored as length-1 tuples. In this particular case, <code class="docutils literal notranslate"><span class="pre">model.transformer.h[-1]</span></code> is a <code class="docutils literal notranslate"><span class="pre">GPTJBlock</span></code> module, which outputs a length-2 tensor. The 0th is the residual stream at the end of the block (i.e. the thing we want in this case).</p>
<details><summary><p>Optional exercise - from the source code page, can you figure out what the second output in this tuple is?</p>
</summary><p>The second output is also a tuple of tensors, of length 2. In the GPT-J source code, they are called <code class="docutils literal notranslate"><span class="pre">present</span></code>. They represent the keys and values which were calculated in this forward pass (as opposed to those that were calculated in an earlier forward pass, and cached by the model). Since we’re only generating one new token, these are just the full keys and values.</p>
</details><p>When debugging, you can call <code class="docutils literal notranslate"><span class="pre">.shape</span></code> on a proxy. This will even work if the proxy represents a tuple of tensors; you’ll get a tuple of all the sizes of these tensors.</p>
<p>You can also use <code class="docutils literal notranslate"><span class="pre">.input</span></code> to access the inputs to a module - this works in the same way (often also stored as a tuple).</p>
</section>
<section id=".save()">
<h4><code class="docutils literal notranslate"><span class="pre">.save()</span></code><a class="headerlink" href="#.save()" title="Permalink to this heading">#</a></h4>
<p>This informs the computation graph to clone the value of a proxy, allowing us to access the value of a proxy after generation.</p>
<p>During processing of the intervention computational graph we are building, when the value of a proxy is no longer ever needed, its value is dereferenced and destroyed. If you’ve saved, then you’ll be able to access the value of the proxy after this happens (i.e. outside the context manager), using the <code class="docutils literal notranslate"><span class="pre">.value</span></code> attribute.</p>
</section>
</section>
<section id="Exercise---visualize-attention-heads">
<h3>Exercise - visualize attention heads<a class="headerlink" href="#Exercise---visualize-attention-heads" title="Permalink to this heading">#</a></h3>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>Difficulty: 🔴🔴⚪⚪⚪
Importance: 🔵🔵🔵⚪⚪

You should spend up to 10-20 minutes on this exercise.
</pre></div>
</div>
<p>That was a lot, so lets put it into practice. Your first task is to extract the attention patterns from the zeroth layer of the transformer, and visualize them using circuitsvis. As a reminder, the syntax for circuitsvis is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cv</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">attention_patterns</span><span class="p">(</span>
    <span class="n">tokens</span><span class="o">=</span><span class="n">tokens</span><span class="p">,</span>
    <span class="n">attention</span><span class="o">=</span><span class="n">attention</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">tokens</span></code> is a list of strings, and <code class="docutils literal notranslate"><span class="pre">attention</span></code> is a tensor of shape <code class="docutils literal notranslate"><span class="pre">(num_heads,</span> <span class="pre">num_tokens,</span> <span class="pre">num_tokens)</span></code>.</p>
<p>If you’re stuck, <a class="reference external" href="https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html">here’s a link</a> to the source code for GPT-J. Look for how the attention patterns are calculated, within the <code class="docutils literal notranslate"><span class="pre">GPTJAttention</span></code> block.</p>
<p><em>Note - this model uses dropout on the attention probabilities, as you’ll probably notice from looking at the source code in the link above. This won’t affect the model’s behaviour because dropout is disabled in inference mode (and using the ``generate`` method always puts a model in inference mode). But it is still a layer which exists in the model, so you can access its input or output just like any other module.</em></p>
<details><summary><p>Aside - inference mode</p>
</summary><p>Dropout is one of the two main layers whose behaviour changes in inference mode (the other is BatchNorm).</p>
<p>If you want to run the model without inference mode, you can wrap your code in <code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">model.forward(inference=False):</span></code>. However, you don’t need to worry about this for the purposes of these exercises.</p>
</details><p>If you’re stuck indexing the model, see the following hint:</p>
<details><summary><p>Hint - what module you should get attention from</p>
</summary><p>You want to extract attention from <code class="docutils literal notranslate"><span class="pre">model.transformer.h[0].attn.attn_dropout.input</span></code>. If you used <code class="docutils literal notranslate"><span class="pre">.output</span></code>, it would give you the same values (although they might differ by a dummy batch dimension). Both of these will return a single tensor, because dropout layers take just one input and return just one output.</p>
</details><div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">)</span> <span class="k">as</span> <span class="n">generator</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="k">as</span> <span class="n">invoker</span><span class="p">:</span>
        <span class="n">attn_patterns</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">attn_dropout</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get string tokens (replacing special character for spaces)</span>
<span class="n">str_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
<span class="n">str_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;Ġ&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">str_tokens</span><span class="p">]</span>

<span class="c1"># Attention patterns (squeeze out the batch dimension)</span>
<span class="n">attn_patterns_value</span> <span class="o">=</span> <span class="n">attn_patterns</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Layer 0 Head Attention Patterns:&quot;</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">attention_patterns</span><span class="p">(</span>
    <span class="n">tokens</span><span class="o">=</span><span class="n">str_tokens</span><span class="p">,</span>
    <span class="n">attention</span><span class="o">=</span><span class="n">attn_patterns_value</span><span class="p">,</span>
<span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Layer 0 Head Attention Patterns:
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<div id="circuits-vis-a9aaae2a-e0e8" style="margin: 15px 0;"/>
    <script crossorigin type="module">
    import { render, AttentionPatterns } from "https://unpkg.com/circuitsvis@1.41.0/dist/cdn/esm.js";
    render(
      "circuits-vis-a9aaae2a-e0e8",
      AttentionPatterns,
      {"tokens": ["The", " E", "iff", "el", " Tower", " is", " in", " the", " city", " of"], "attention": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.829063355922699, 0.1709366738796234, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10449792444705963, 0.8631790280342102, 0.03232300281524658, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.002194455824792385, 0.01044239941984415, 0.9758203029632568, 0.011542831547558308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01957514137029648, 0.0005238918820396066, 0.8991208672523499, 0.07888323813676834, 0.0018968796357512474, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09968297183513641, 0.002919913735240698, 0.0023292850237339735, 0.008620481006801128, 0.11867374181747437, 0.7677736282348633, 0.0, 0.0, 0.0, 0.0], [0.1369725465774536, 0.03587929904460907, 0.009825349785387516, 0.013134374283254147, 0.036710482090711594, 0.520531415939331, 0.24694649875164032, 0.0, 0.0, 0.0], [0.09312178194522858, 0.008496217429637909, 0.0036471723578870296, 0.006210173014551401, 0.007328539155423641, 0.2238079458475113, 0.0488654300570488, 0.6085227727890015, 0.0, 0.0], [0.022006427869200706, 0.005823437124490738, 0.0057095373049378395, 0.003698136657476425, 0.05718507990241051, 0.18564170598983765, 0.06623344123363495, 0.5254097580909729, 0.12829247117042542, 0.0], [0.004313651472330093, 0.000997802708297968, 0.00022000973694957793, 0.0013666593004018068, 0.005537914112210274, 0.03576671704649925, 0.00845794752240181, 0.06083754450082779, 0.7149895429611206, 0.1675121784210205]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7337515354156494, 0.2662484645843506, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01838100515305996, 0.9661518931388855, 0.015467054210603237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.28406479954719543, 0.4624061584472656, 0.08217735588550568, 0.17135171592235565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.29254311323165894, 0.025398923084139824, 0.23610815405845642, 0.11693140864372253, 0.3290184438228607, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2333838790655136, 0.06438977271318436, 0.054571252316236496, 0.04971354082226753, 0.4808805286884308, 0.11706104129552841, 0.0, 0.0, 0.0, 0.0], [0.1657487154006958, 0.036285143345594406, 0.039978135377168655, 0.03913314640522003, 0.4151957333087921, 0.12842589616775513, 0.17523330450057983, 0.0, 0.0, 0.0], [0.18088535964488983, 0.027216654270887375, 0.0364697128534317, 0.025995688512921333, 0.1539277285337448, 0.19731678068637848, 0.16624368727207184, 0.2119443267583847, 0.0, 0.0], [0.061875078827142715, 0.01634238287806511, 0.053551092743873596, 0.02586761675775051, 0.3887181580066681, 0.02492697909474373, 0.07002407312393188, 0.07096059620380402, 0.2877340316772461, 0.0], [0.1078551709651947, 0.010135271586477757, 0.03009823150932789, 0.015300117433071136, 0.13075672090053558, 0.06642581522464752, 0.08435479551553726, 0.1750207394361496, 0.22528469562530518, 0.1547684371471405]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9375947713851929, 0.06240519881248474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5738810300827026, 0.3882512152194977, 0.037867821753025055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6073166728019714, 0.1819608360528946, 0.10958211869001389, 0.10114036500453949, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2613397538661957, 0.4545661211013794, 0.04196810722351074, 0.10260388255119324, 0.13952212035655975, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7512133717536926, 0.030134251341223717, 0.03863786906003952, 0.03444979339838028, 0.05264578387141228, 0.09291895478963852, 0.0, 0.0, 0.0, 0.0], [0.08130466192960739, 0.008510569110512733, 0.011866848915815353, 0.0038354594726115465, 0.047947682440280914, 0.13581959903240204, 0.7107151746749878, 0.0, 0.0, 0.0], [0.16110482811927795, 0.005045980680733919, 0.00649917172268033, 0.006183331366628408, 0.004202307667583227, 0.006440227385610342, 0.09915072470903397, 0.7113734483718872, 0.0, 0.0], [0.023170091211795807, 0.00899879913777113, 0.03188159689307213, 0.010966754518449306, 0.7636092901229858, 0.008237296715378761, 0.07699004560709, 0.033955082297325134, 0.042191095650196075, 0.0], [0.116624616086483, 0.004521339666098356, 0.005119765643030405, 0.006424614693969488, 0.010177543386816978, 0.008108649402856827, 0.19665247201919556, 0.48110663890838623, 0.012579265050590038, 0.15868504345417023]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9873823523521423, 0.012617615982890129, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.705640971660614, 0.03949311003088951, 0.2548658847808838, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5494459271430969, 0.027415193617343903, 0.19888116419315338, 0.22425773739814758, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13006022572517395, 0.023235522210597992, 0.37218791246414185, 0.35988831520080566, 0.11462795734405518, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1305249035358429, 0.021449685096740723, 0.12894372642040253, 0.27416539192199707, 0.057148270308971405, 0.38776805996894836, 0.0, 0.0, 0.0, 0.0], [0.25049659609794617, 0.007087950129061937, 0.09113315492868423, 0.1751839518547058, 0.07337385416030884, 0.13667626678943634, 0.2660481929779053, 0.0, 0.0, 0.0], [0.11081762611865997, 0.010958189144730568, 0.04083585739135742, 0.06034961715340614, 0.032030925154685974, 0.09623461961746216, 0.19093474745750427, 0.4578384757041931, 0.0, 0.0], [0.05363744869828224, 0.025838658213615417, 0.09583628177642822, 0.18366430699825287, 0.05054944008588791, 0.133483424782753, 0.23405790328979492, 0.17247045040130615, 0.05046214163303375, 0.0], [0.0948793813586235, 0.015751123428344727, 0.05621325969696045, 0.10723130404949188, 0.039503198117017746, 0.10456932336091995, 0.09332013130187988, 0.25014761090278625, 0.06267370283603668, 0.1757110208272934]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8586137890815735, 0.14138615131378174, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5009570717811584, 0.17692223191261292, 0.32212066650390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01860657148063183, 0.11020754277706146, 0.8365734219551086, 0.03461247682571411, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4734065532684326, 0.14704622328281403, 0.05715098977088928, 0.25222980976104736, 0.07016641646623611, 0.0, 0.0, 0.0, 0.0, 0.0], [0.49333417415618896, 0.008234936743974686, 0.008012527599930763, 0.021872591227293015, 0.03967061638832092, 0.428875207901001, 0.0, 0.0, 0.0, 0.0], [0.08060485869646072, 0.0033893154468387365, 0.002171718282625079, 0.0026488034054636955, 0.036696918308734894, 0.7740417718887329, 0.10044671595096588, 0.0, 0.0, 0.0], [0.08436581492424011, 0.008884111419320107, 0.006725000683218241, 0.013129651546478271, 0.01784602180123329, 0.1932034194469452, 0.1695568710565567, 0.5062890648841858, 0.0, 0.0], [0.13414521515369415, 0.014364570379257202, 0.00585120590403676, 0.01126515306532383, 0.00909710954874754, 0.16818936169147491, 0.3251395523548126, 0.2252425104379654, 0.10670536756515503, 0.0], [2.452119588269852e-05, 1.9645636712084524e-05, 0.00011441295646363869, 3.1429917726200074e-05, 5.5669035646133125e-05, 0.0013380550080910325, 0.02684668079018593, 0.0024312452878803015, 0.9657877087593079, 0.0033504932653158903]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5130534768104553, 0.48694655299186707, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3975912034511566, 0.5228423476219177, 0.07956639677286148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23553597927093506, 0.22942191362380981, 0.20850297808647156, 0.32653912901878357, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2904796898365021, 0.1287231743335724, 0.2951997220516205, 0.12991349399089813, 0.15568388998508453, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7625570297241211, 0.027590621262788773, 0.027350928634405136, 0.02580930106341839, 0.03493652120232582, 0.12175561487674713, 0.0, 0.0, 0.0, 0.0], [0.4462546408176422, 0.008537102490663528, 0.028795866295695305, 0.032935936003923416, 0.04105943813920021, 0.3229061961174011, 0.11951085925102234, 0.0, 0.0, 0.0], [0.28493931889533997, 0.011074136942625046, 0.009190265089273453, 0.019453972578048706, 0.026987304911017418, 0.19498902559280396, 0.349621057510376, 0.10374491661787033, 0.0, 0.0], [0.05542522296309471, 0.0641314908862114, 0.03741443157196045, 0.0557597354054451, 0.5587040185928345, 0.0592208206653595, 0.06786541640758514, 0.040136002004146576, 0.06134287267923355, 0.0], [0.18957604467868805, 0.007861909456551075, 0.010476202704012394, 0.012908213771879673, 0.010497424751520157, 0.1034943088889122, 0.07343605905771255, 0.4299052953720093, 0.02457478456199169, 0.13726970553398132]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8735913634300232, 0.1264086216688156, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4011150896549225, 0.5105474591255188, 0.08833743631839752, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.027618370950222015, 0.05665247142314911, 0.8718303442001343, 0.04389885812997818, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07733327895402908, 0.3653131127357483, 0.11332374811172485, 0.32430601119995117, 0.1197238638997078, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6556437611579895, 0.06300179660320282, 0.05083257704973221, 0.0788804367184639, 0.05136638134717941, 0.10027503967285156, 0.0, 0.0, 0.0, 0.0], [0.18654292821884155, 0.10325082391500473, 0.06530754268169403, 0.11005351692438126, 0.07516941428184509, 0.1905387043952942, 0.26913711428642273, 0.0, 0.0, 0.0], [0.09667760133743286, 0.04255342483520508, 0.023608200252056122, 0.056543491780757904, 0.048377860337495804, 0.12254257500171661, 0.33910518884658813, 0.270591676235199, 0.0, 0.0], [0.06725477427244186, 0.07902779430150986, 0.037737708538770676, 0.05301019921898842, 0.029360635206103325, 0.07956801354885101, 0.4394588768482208, 0.13144783675670624, 0.08313417434692383, 0.0], [0.06725261360406876, 0.03356018289923668, 0.02768505923449993, 0.05884742736816406, 0.03936830163002014, 0.08749471604824066, 0.1688593029975891, 0.1470056027173996, 0.1658822000026703, 0.20404452085494995]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8174959421157837, 0.1825040578842163, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11346088349819183, 0.8076116442680359, 0.07892750203609467, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.017064860090613365, 0.20547014474868774, 0.5517476797103882, 0.22571733593940735, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.011147839948534966, 0.02403853088617325, 0.7872782945632935, 0.0936041995882988, 0.0839310735464096, 0.0, 0.0, 0.0, 0.0, 0.0], [0.007123135961592197, 0.03321104496717453, 0.11185067147016525, 0.11278776079416275, 0.44324788451194763, 0.291779488325119, 0.0, 0.0, 0.0, 0.0], [0.006628019735217094, 0.015841038897633553, 0.023362331092357635, 0.028282128274440765, 0.07155666500329971, 0.7880684733390808, 0.06626132130622864, 0.0, 0.0, 0.0], [0.0004611426265910268, 0.0012083295732736588, 0.001096363179385662, 0.0020610291976481676, 0.004222366493195295, 0.052768509835004807, 0.8986179232597351, 0.03956422582268715, 0.0, 0.0], [0.0044714403338730335, 0.0030466821044683456, 0.025206604972481728, 0.0184777844697237, 0.028084393590688705, 0.12872426211833954, 0.13313446938991547, 0.4756620228290558, 0.18319226801395416, 0.0], [0.00091141666052863, 0.0009552808478474617, 0.008818528614938259, 0.006785875651985407, 0.01171346940100193, 0.05196182429790497, 0.09358130395412445, 0.2109144628047943, 0.426074355840683, 0.18828347325325012]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3345666229724884, 0.665433406829834, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2124827653169632, 0.46171835064888, 0.325798898935318, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21691139042377472, 0.363395094871521, 0.2873225510120392, 0.1323709785938263, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14811821281909943, 0.3364785313606262, 0.20350439846515656, 0.07528091967105865, 0.23661790788173676, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08503736555576324, 0.18161176145076752, 0.24255557358264923, 0.1580277532339096, 0.19204944372177124, 0.14071807265281677, 0.0, 0.0, 0.0, 0.0], [0.1245218813419342, 0.11159589886665344, 0.1399824321269989, 0.08270397782325745, 0.17941509187221527, 0.2354891002178192, 0.12629158794879913, 0.0, 0.0, 0.0], [0.028029654175043106, 0.06734573096036911, 0.09047392755746841, 0.048100363463163376, 0.1489553302526474, 0.1657056361436844, 0.4358976483345032, 0.01549173891544342, 0.0, 0.0], [0.08234745264053345, 0.2559697926044464, 0.08787793666124344, 0.08200085908174515, 0.24880775809288025, 0.08278200775384903, 0.0438232496380806, 0.0316946879029274, 0.0846962258219719, 0.0], [0.0520622581243515, 0.08785209059715271, 0.08251846581697464, 0.06066236272454262, 0.09179577976465225, 0.10067924857139587, 0.22894617915153503, 0.04295729473233223, 0.11332003772258759, 0.13920623064041138]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6521740555763245, 0.3478259742259979, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.47260844707489014, 0.4507277309894562, 0.07666382938623428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.45525529980659485, 0.1926717460155487, 0.05873100459575653, 0.2933419644832611, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4720485210418701, 0.14024792611598969, 0.06089228391647339, 0.2702023386955261, 0.05660896375775337, 0.0, 0.0, 0.0, 0.0, 0.0], [0.32822540402412415, 0.1246102899312973, 0.04554545879364014, 0.15255127847194672, 0.07841417193412781, 0.2706533968448639, 0.0, 0.0, 0.0, 0.0], [0.2873716652393341, 0.08126501739025116, 0.020196959376335144, 0.060919586569070816, 0.056354962289333344, 0.21469873189926147, 0.27919304370880127, 0.0, 0.0, 0.0], [0.20940187573432922, 0.09117225557565689, 0.03148096054792404, 0.06984073668718338, 0.044023770838975906, 0.08230166137218475, 0.1934521496295929, 0.2783266305923462, 0.0, 0.0], [0.19073554873466492, 0.024706248193979263, 0.004702771548181772, 0.009100218303501606, 0.011310776695609093, 0.261827677488327, 0.20821835100650787, 0.21922342479228973, 0.0701749324798584, 0.0], [0.1045900210738182, 0.01886243000626564, 0.0067127360962331295, 0.010519926436245441, 0.014393172226846218, 0.12164101004600525, 0.14099566638469696, 0.279011607170105, 0.04852485656738281, 0.2547486126422882]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5327021479606628, 0.4672977924346924, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2894168496131897, 0.428524374961853, 0.2820587754249573, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0027006505988538265, 0.00782750267535448, 0.9693534970283508, 0.020118290558457375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24669340252876282, 0.17147505283355713, 0.23301292955875397, 0.1221165582537651, 0.22670206427574158, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2555573284626007, 0.03965854272246361, 0.028971193358302116, 0.020663244649767876, 0.011494029313325882, 0.6436557173728943, 0.0, 0.0, 0.0, 0.0], [0.20796450972557068, 0.03158773481845856, 0.023883530870079994, 0.030746977776288986, 0.015544509515166283, 0.20087400078773499, 0.48939865827560425, 0.0, 0.0, 0.0], [0.07701205462217331, 0.010727901943027973, 0.005023232661187649, 0.008057613857090473, 0.0012516246642917395, 0.029886292293667793, 0.12402784079313278, 0.7440134286880493, 0.0, 0.0], [0.09852539747953415, 0.08819779008626938, 0.07811672240495682, 0.033274635672569275, 0.26161882281303406, 0.052245840430259705, 0.1673254519701004, 0.09561891853809357, 0.12507639825344086, 0.0], [0.0018261066870763898, 0.0012838448164984584, 0.0008565881289541721, 0.0016095238970592618, 0.004616647958755493, 0.002421535551548004, 0.003519008168950677, 0.005768938921391964, 0.9421509504318237, 0.035946886986494064]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4359164834022522, 0.5640835165977478, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.34310558438301086, 0.193734273314476, 0.46316006779670715, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18442295491695404, 0.08524700254201889, 0.2432430535554886, 0.4870869815349579, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21567164361476898, 0.13079501688480377, 0.24097099900245667, 0.2548925578594208, 0.1576698124408722, 0.0, 0.0, 0.0, 0.0, 0.0], [0.050237834453582764, 0.03873935341835022, 0.050004757940769196, 0.04733559116721153, 0.03742910549044609, 0.776253342628479, 0.0, 0.0, 0.0, 0.0], [0.05114845559000969, 0.04080699756741524, 0.06932128220796585, 0.03621926158666611, 0.03314264863729477, 0.1610209047794342, 0.6083404421806335, 0.0, 0.0, 0.0], [0.025111332535743713, 0.03258886560797691, 0.044793300330638885, 0.03725753352046013, 0.021802028641104698, 0.04561690241098404, 0.07765314728021622, 0.7151768803596497, 0.0, 0.0], [0.07588838785886765, 0.055973608046770096, 0.10367074608802795, 0.04044070094823837, 0.04906556010246277, 0.16095605492591858, 0.14304016530513763, 0.3384029269218445, 0.03256189450621605, 0.0], [0.040442340075969696, 0.06247076019644737, 0.12395961582660675, 0.08377137780189514, 0.06365847587585449, 0.07885753363370895, 0.07767991721630096, 0.16915485262870789, 0.04937927424907684, 0.2506258487701416]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8666063547134399, 0.13339364528656006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7037226557731628, 0.08050073683261871, 0.21577665209770203, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5062465667724609, 0.06629222631454468, 0.06981618702411652, 0.35764503479003906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.37740546464920044, 0.028806498274207115, 0.1494835913181305, 0.16624043881893158, 0.27806395292282104, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21135656535625458, 0.042256638407707214, 0.13441215455532074, 0.10484889149665833, 0.3652103841304779, 0.14191538095474243, 0.0, 0.0, 0.0, 0.0], [0.10934919118881226, 0.02916078269481659, 0.10378804802894592, 0.07588699460029602, 0.5279759168624878, 0.05085797235369682, 0.10298112034797668, 0.0, 0.0, 0.0], [0.07791783660650253, 0.032187219709157944, 0.03131778538227081, 0.05185353383421898, 0.08924010396003723, 0.04645169898867607, 0.1844736784696579, 0.48655813932418823, 0.0, 0.0], [0.14273370802402496, 0.008835560642182827, 0.035214927047491074, 0.022769751027226448, 0.18999800086021423, 0.02845376916229725, 0.03436779975891113, 0.23312732577323914, 0.3044991195201874, 0.0], [0.06128959730267525, 0.017167722806334496, 0.07880358397960663, 0.03203364461660385, 0.2612563371658325, 0.018978141248226166, 0.03086722083389759, 0.11583200842142105, 0.23447981476783752, 0.14929191768169403]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8212168216705322, 0.17878317832946777, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3396758735179901, 0.5219749808311462, 0.13834913074970245, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04702940583229065, 0.19365988671779633, 0.6941652297973633, 0.06514539569616318, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0762493908405304, 0.04433031007647514, 0.04686227813363075, 0.8197378516197205, 0.012820110656321049, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2521655559539795, 0.07540450245141983, 0.0806838646531105, 0.11423364281654358, 0.168213352560997, 0.3092990219593048, 0.0, 0.0, 0.0, 0.0], [0.09420900791883469, 0.01816960610449314, 0.028360724449157715, 0.03727706894278526, 0.08318649977445602, 0.5231131315231323, 0.21568387746810913, 0.0, 0.0, 0.0], [0.07451336085796356, 0.02072478085756302, 0.023791689425706863, 0.0411953404545784, 0.053938958793878555, 0.26805758476257324, 0.19976764917373657, 0.318010538816452, 0.0, 0.0], [0.03059406764805317, 0.012348110787570477, 0.00996171124279499, 0.006671534851193428, 0.0034022685140371323, 0.06109631434082985, 0.1632358729839325, 0.6863061785697937, 0.026384005323052406, 0.0], [0.025209790095686913, 0.005476289428770542, 0.005883364472538233, 0.007687622681260109, 0.014250179752707481, 0.12825602293014526, 0.1338292509317398, 0.3083134591579437, 0.15664087235927582, 0.21445314586162567]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9453611373901367, 0.05463884398341179, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7012256979942322, 0.16970087587833405, 0.12907345592975616, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7403304576873779, 0.11875211447477341, 0.036210622638463974, 0.10470681637525558, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12056373804807663, 0.08851055055856705, 0.15055081248283386, 0.5414496064186096, 0.09892535209655762, 0.0, 0.0, 0.0, 0.0, 0.0], [0.28596463799476624, 0.0679699182510376, 0.025052212178707123, 0.041540343314409256, 0.17584450542926788, 0.4036283493041992, 0.0, 0.0, 0.0, 0.0], [0.2957458198070526, 0.04547145962715149, 0.02355162426829338, 0.028333498165011406, 0.07833416759967804, 0.24397577345371246, 0.2845875918865204, 0.0, 0.0, 0.0], [0.08486074954271317, 0.025896485894918442, 0.01629001647233963, 0.008989562280476093, 0.014236284419894218, 0.18315383791923523, 0.23715601861476898, 0.42941710352897644, 0.0, 0.0], [0.04092155024409294, 0.048806264996528625, 0.01738102175295353, 0.010911516845226288, 0.2677488923072815, 0.06343573331832886, 0.30654680728912354, 0.126474991440773, 0.11777325719594955, 0.0], [0.14077208936214447, 0.02675667591392994, 0.005762417800724506, 0.009632841683924198, 0.013735946267843246, 0.07092709839344025, 0.1094210147857666, 0.35359957814216614, 0.0213389303535223, 0.24805346131324768]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9203739762306213, 0.07962600141763687, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.20258858799934387, 0.6306097507476807, 0.16680166125297546, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.002432319801300764, 0.0018929652869701385, 0.9721331596374512, 0.02354150265455246, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2419133484363556, 0.005510878283530474, 0.17049086093902588, 0.2913920283317566, 0.29069286584854126, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24400991201400757, 0.009351830929517746, 0.0177416130900383, 0.03455575555562973, 0.009871178306639194, 0.6844696402549744, 0.0, 0.0, 0.0, 0.0], [0.14732977747917175, 0.015364331193268299, 0.004563696216791868, 0.011315342970192432, 0.043744493275880814, 0.5253489017486572, 0.2523334324359894, 0.0, 0.0, 0.0], [0.07099173963069916, 0.002983162645250559, 0.008780461736023426, 0.010933419689536095, 0.012722334824502468, 0.13144055008888245, 0.16819895803928375, 0.5939492583274841, 0.0, 0.0], [0.034700773656368256, 0.0020962192211300135, 0.003587249666452408, 0.004411224741488695, 0.3381350636482239, 0.19166678190231323, 0.040754564106464386, 0.20953278243541718, 0.17511534690856934, 0.0], [0.04594598338007927, 0.0035846976097673178, 0.0035615027882158756, 0.01369266677647829, 0.022732658311724663, 0.04340611398220062, 0.04250757768750191, 0.2669268548488617, 0.14452892541885376, 0.413112998008728]]]}
    )
    </script></div>
</div>
<details><summary><p>Solution (and explanation)</p>
</summary><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">)</span> <span class="k">as</span> <span class="n">generator</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="k">as</span> <span class="n">invoker</span><span class="p">:</span>
        <span class="n">attn_patterns</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">attn_dropout</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="n">str_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>

<span class="c1"># Attention patterns (squeeze out the batch dimension)</span>
<span class="n">attn_patterns</span> <span class="o">=</span> <span class="n">attn_patterns</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Layer 0 Head Attention Patterns:&quot;</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">attention_patterns</span><span class="p">(</span>
    <span class="n">tokens</span><span class="o">=</span><span class="n">str_tokens</span><span class="p">,</span>
    <span class="n">attention</span><span class="o">=</span><span class="n">attn_patterns</span><span class="p">,</span>
<span class="p">))</span>
</pre></div>
</div>
<p>Explanation:</p>
<ul class="simple">
<li><p>Within the context managers:</p>
<ul>
<li><p>We access the attention patterns by taking the input to the <code class="docutils literal notranslate"><span class="pre">attn_dropout</span></code>.</p>
<ul>
<li><p>From the GPT-J source code, we can see that the attention weights are calculated by standard torch functions (and an unnamed <code class="docutils literal notranslate"><span class="pre">nn.Softmax</span></code> module) from the key and query vectors, and are then passed through the dropout layer before being used to calculate the attention layer output. So by accessing the input to the dropdown layer, we get the attention weights before dropout is applied.</p></li>
<li><p>Because of the previously discussed point about dropout not working in inference mode, we could also use the output of <code class="docutils literal notranslate"><span class="pre">attn_dropout</span></code>, and get the same values.</p></li>
</ul>
</li>
<li><p>We use the <code class="docutils literal notranslate"><span class="pre">.save()</span></code> method to save the attention patterns (as an object).</p></li>
</ul>
</li>
<li><p>Outside of the context managers:</p>
<ul>
<li><p>We use the <code class="docutils literal notranslate"><span class="pre">tokenize</span></code> method to tokenize the prompt.</p></li>
<li><p>We use the <code class="docutils literal notranslate"><span class="pre">.value</span></code> to access the actual value of the intervention proxy <code class="docutils literal notranslate"><span class="pre">attn_patterns</span></code>.</p>
<ul>
<li><p>This returns a tuple of length-1, so we index into it to get the actual tensor, then squeeze to remove the batch dimension.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</details><p>As an optional bonus exercise, you can verify for yourself that these are the correct attention patterns, by calculating them from scratch using the key and query vectors. Using <code class="docutils literal notranslate"><span class="pre">model.transformer.h[0].attn.q_proj.output</span></code> will give you the query vectors, and <code class="docutils literal notranslate"><span class="pre">k_proj</span></code> for the key vectors. However, one thing to be wary of is that GPT-J uses <strong>rotary embeddings</strong>, which makes the computation of attention patterns from keys and queries a bit harder than it would otherwise be. See
<a class="reference external" href="https://blog.eleuther.ai/rotary-embeddings/">here</a> for an in-depth discussion of rotary embeddings, and <a class="reference external" href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#q=rotary">here</a> for some rough intuitions.</p>
</section>
</section>
</section>
<section id="id2">
<h1>2️⃣ Task-encoding hidden states<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h1>
<p>(Note - this section structurally follows section 2.1 of the function vectors paper).</p>
<p>We begin with the following question:</p>
<blockquote>
<div><p>When a transformer processes an ICL (in-context-learning) prompt with exemplars demonstrating task <span class="math notranslate nohighlight">\(T\)</span>, do any hidden states encode the task itself?</p>
</div></blockquote>
<p>Throughout these exercises, we’ll be focusing on the <strong>antonyms task</strong>. In other words, given a prompt which includes a bunch of antonym pairs, ending with a single word, what causes the model to complete this prompt with an antonym? Is there a residual stream state that encodes the “antonym task”?</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>Difficulty: 🔴🔴🔴🔴⚪
Importance: 🔵⚪⚪⚪⚪

You should spend up to 10-30 minutes on this exercise - depending on your familiarity with the OpenAI Python API.
</pre></div>
</div>
<p>We’ve provided you a list of word pairs, in the file <code class="docutils literal notranslate"><span class="pre">data/antonym_pairs.txt</span></code>. <strong>Optionally, you can just skip this exercise, and run the code below to load these words in.</strong></p>
<p>Alternatively, if you want to run experiments like the ones in this paper, it can be good practice to learn how to generate prompts from GPT-4 or other models (this is how we generated the data for this exercise). To do this, you can fill in the <code class="docutils literal notranslate"><span class="pre">generate_dataset</span></code> function below, which should query GPT-4 and get a list of antonym pairs. See <a class="reference external" href="https://platform.openai.com/docs/guides/gpt/chat-completions-api">here</a> for a guide to using the chat completions API, if you haven’t already used it.</p>
<p>Use the two dropdowns below (in order) for some guidance.</p>
<details><summary><p>Getting started</p>
</summary><p>Here is a recommended template:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">response</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">ChatCompletion</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">antonym_task</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">start_of_response</span><span class="p">},</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">antonym_task</span></code> explains the antonym task, and <code class="docutils literal notranslate"><span class="pre">start_of_respose</span></code> gives the model a prompt to start from (e.g. “Sure, here are some antonyms: …”), to guide its subsequent behaviour.</p>
</details><details><summary><p>Getting started</p>
</summary><p>Here is an template you might want to use for the actual request:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">example_antonyms</span> <span class="o">=</span> <span class="s2">&quot;old:young, top:bottom, awake:asleep, future:past, &quot;</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">ChatCompletion</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Give me </span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s2"> examples of antonym pairs. They should be obvious, i.e. each word should be associated with a single correct antonym.&quot;</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Sure! Here are </span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s2"> pairs of antonyms: </span><span class="si">{</span><span class="n">example_antonyms</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">},</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the function argument. Note that we’ve provided a few example antonyms, and appended them to the start of GPT4’s completion. This is a classic trick to guide the rest of the output (in fact, it’s commonly used in adversarial attacks).</p>
</details><p>Note - it’s possible that not all the antonyms returned will be solvable by GPT-J. In this section, we won’t worry too much about this. When it comes to testing out our zero-shot intervention, we’ll make sure to only use cases where GPT-J can actually solve it.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">openai_key</span> <span class="o">=</span> <span class="s2">&quot;insert your key here!&quot;</span>
<span class="n">openai</span><span class="o">.</span><span class="n">api_key</span> <span class="o">=</span> <span class="n">openai_key</span>

<span class="k">def</span> <span class="nf">generate_dataset</span><span class="p">(</span><span class="n">N</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>

    <span class="k">assert</span> <span class="n">openai_key</span> <span class="o">!=</span> <span class="s2">&quot;insert your key here!&quot;</span><span class="p">,</span> <span class="s2">&quot;Please insert your own key before running this function!&quot;</span>

    <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="c1"># Define a few examples (for our dataset, and for our prompt)</span>
    <span class="n">example_antonyms</span> <span class="o">=</span> <span class="s2">&quot;old:young, top:bottom, awake:asleep, future:past, &quot;</span>

    <span class="c1"># Use openai&#39;s api to generate examples. We prepend the example antonyms to the assistant&#39;s response, to both</span>
    <span class="c1"># make sure the query is successful, and so that the assistant returns words in the same syntax as the examples.</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">ChatCompletion</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4&quot;</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Give me </span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s2"> examples of antonym pairs. They should be obvious, i.e. each word should be associated with a single correct antonym.&quot;</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Sure! Here are </span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s2"> pairs of antonyms satiisfying this specification: </span><span class="si">{</span><span class="n">example_antonyms</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">},</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="c1"># Add our examples to the response</span>
    <span class="n">response_text</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">example_antonyms</span> <span class="o">+</span> <span class="n">response</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;message&quot;</span><span class="p">][</span><span class="s2">&quot;content&quot;</span><span class="p">]</span>

    <span class="c1"># Create word pairs, by splitting on commas and colons</span>
    <span class="n">word_pairs</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_pair</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;:&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">word_pair</span> <span class="ow">in</span> <span class="n">response_text</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s2">&quot;.</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;, &quot;</span><span class="p">)]</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Finished in </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> seconds.&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">word_pairs</span>


<span class="c1"># WORD_PAIRS = generate_dataset(100)</span>

<span class="c1"># # save the word pairs in a text file</span>
<span class="c1"># with open(root / &quot;data&quot; / &quot;antonym_pairs.txt&quot;, &quot;w&quot;) as f:</span>
<span class="c1">#     for word_pair in WORD_PAIRS:</span>
<span class="c1">#         f.write(f&quot;{word_pair[0]} {word_pair[1]}\n&quot;)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load the word pairs from the text file</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">root</span> <span class="o">/</span> <span class="s2">&quot;data&quot;</span> <span class="o">/</span> <span class="s2">&quot;antonym_pairs.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">WORD_PAIRS</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">()]</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">WORD_PAIRS</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[&#39;old&#39;, &#39;young&#39;],
 [&#39;top&#39;, &#39;bottom&#39;],
 [&#39;awake&#39;, &#39;asleep&#39;],
 [&#39;future&#39;, &#39;past&#39;],
 [&#39;beginning&#39;, &#39;end&#39;],
 [&#39;volunteer&#39;, &#39;compel&#39;],
 [&#39;best&#39;, &#39;worst&#39;],
 [&#39;big&#39;, &#39;small&#39;],
 [&#39;boring&#39;, &#39;exciting&#39;],
 [&#39;brave&#39;, &#39;cowardly&#39;]]
</pre></div></div>
</div>
<section id="Antonym-Dataset">
<h2>Antonym Dataset<a class="headerlink" href="#Antonym-Dataset" title="Permalink to this heading">#</a></h2>
<p>To handle this list of word pairs, we’ve given you some helpful classes.</p>
<p>Firstly, there’s the <code class="docutils literal notranslate"><span class="pre">AntonymSequence</span></code> class, which takes in a list of word pairs and contains methods for constructing a prompt (and completion) from these words. Run the code below to see how it works.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AntonymSequence</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Class to store a single antonym sequence.</span>

<span class="sd">    Uses the default template &quot;Q: {x}\nA: {y}&quot; (with separate pairs split by &quot;\n\n&quot;).</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word_pairs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_pairs</span> <span class="o">=</span> <span class="n">word_pairs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">word_pairs</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">word_pairs</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_pairs</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">prompt</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;Returns the prompt, which contains all but the second element in the last word pair.&#39;&#39;&#39;</span>
        <span class="n">p</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s2">&quot;Q: </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="se">\n</span><span class="s2">A: </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_pairs</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">p</span><span class="p">[:</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">completion</span><span class="p">())]</span>

    <span class="k">def</span> <span class="nf">completion</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;Returns the second element in the last word pair (with padded space).&#39;&#39;&#39;</span>
        <span class="k">return</span> <span class="s2">&quot; &quot;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;Prints a readable string representation of the prompt &amp; completion (indep of template).&#39;&#39;&#39;</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s1">)&#39;</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="bp">self</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> -&gt;&quot;</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s2">&quot;, &quot;</span><span class="p">)</span>


<span class="n">word_list</span> <span class="o">=</span> <span class="p">[[</span><span class="s2">&quot;hot&quot;</span><span class="p">,</span> <span class="s2">&quot;cold&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;yes&quot;</span><span class="p">,</span> <span class="s2">&quot;no&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;in&quot;</span><span class="p">,</span> <span class="s2">&quot;out&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;up&quot;</span><span class="p">,</span> <span class="s2">&quot;down&quot;</span><span class="p">]]</span>
<span class="n">seq</span> <span class="o">=</span> <span class="n">AntonymSequence</span><span class="p">(</span><span class="n">word_list</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tuple-representation of the sequence:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Actual prompt, which will be fed into the model:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">seq</span><span class="o">.</span><span class="n">prompt</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Tuple-representation of the sequence:
(hot, cold), (yes, no), (in, out), up -&gt;

Actual prompt, which will be fed into the model:
Q: hot
A: cold

Q: yes
A: no

Q: in
A: out

Q: up
A:
</pre></div></div>
</div>
<p>Secondly, we have the <code class="docutils literal notranslate"><span class="pre">AntonymDataset</span></code> class. This is also fed a word pair list, and it has methods for generating batches of prompts and completions. It can generate both clean prompts (where each pair is actually an antonym pair) and corrupted prompts (where the answers for each pair are randomly chosen from the dataset).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AntonymDataset</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Dataset to create antonym pair prompts, in ICL task format. We use random seeds for consistency</span>
<span class="sd">    between the corrupted and clean datasets.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        word_pairs:</span>
<span class="sd">            list of ICL task, e.g. [[&quot;old&quot;, &quot;young&quot;], [&quot;top&quot;, &quot;bottom&quot;], ...] for the antonym task</span>
<span class="sd">        size:</span>
<span class="sd">            number of prompts to generate</span>
<span class="sd">        n_prepended:</span>
<span class="sd">            number of antonym pairs before the single-word ICL task</span>
<span class="sd">        bidirectional:</span>
<span class="sd">            if True, then we also consider the reversed antonym pairs</span>
<span class="sd">        corrupted:</span>
<span class="sd">            if True, then the second word in each pair is replaced with a random word</span>
<span class="sd">        seed:</span>
<span class="sd">            random seed, for consistency &amp; reproducibility</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">word_pairs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
        <span class="n">size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">n_prepended</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">bidirectional</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">corrupted</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">assert</span> <span class="n">n_prepended</span><span class="o">+</span><span class="mi">1</span> <span class="o">&lt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_pairs</span><span class="p">),</span> <span class="s2">&quot;Not enough antonym pairs in dataset to create prompt.&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">word_pairs</span> <span class="o">=</span> <span class="n">word_pairs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word_pair</span> <span class="ow">in</span> <span class="n">word_pairs</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_pair</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_prepended</span> <span class="o">=</span> <span class="n">n_prepended</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bidirectional</span> <span class="o">=</span> <span class="n">bidirectional</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">corrupted</span> <span class="o">=</span> <span class="n">corrupted</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">seqs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">completions</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Generate the dataset (by choosing random antonym pairs, and constructing `AntonymSequence` objects)</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span> <span class="o">+</span> <span class="n">n</span><span class="p">)</span>
            <span class="n">random_pairs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">word_pairs</span><span class="p">),</span> <span class="n">n_prepended</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">random_orders</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">n_prepended</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span><span class="p">(</span><span class="n">bidirectional</span><span class="p">):</span> <span class="n">random_orders</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">word_pairs</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">word_pairs</span><span class="p">[</span><span class="n">pair</span><span class="p">][::</span><span class="n">order</span><span class="p">]</span> <span class="k">for</span> <span class="n">pair</span><span class="p">,</span> <span class="n">order</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">random_pairs</span><span class="p">,</span> <span class="n">random_orders</span><span class="p">)]</span>
            <span class="k">if</span> <span class="n">corrupted</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word_pairs</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
                    <span class="n">word_pairs</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">word_list</span><span class="p">)</span>
            <span class="n">seq</span> <span class="o">=</span> <span class="n">AntonymSequence</span><span class="p">(</span><span class="n">word_pairs</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">seqs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">seq</span><span class="o">.</span><span class="n">prompt</span><span class="p">())</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">seq</span><span class="o">.</span><span class="n">completion</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">create_corrupted_dataset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;Creates a corrupted version of the dataset (with same random seed).&#39;&#39;&#39;</span>
        <span class="k">return</span> <span class="n">AntonymDataset</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">word_pairs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_prepended</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">seqs</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</pre></div>
</div>
</div>
<p>You can see how this dataset works below. <strong>Note that the correct completions have a prepended space</strong>, because this is how the antonym prompts are structured - the answers are tokenized as <code class="docutils literal notranslate"><span class="pre">&quot;A:</span> <span class="pre">answer&quot;</span> <span class="pre">-&gt;</span> <span class="pre">[&quot;A&quot;,</span> <span class="pre">&quot;:&quot;,</span> <span class="pre">&quot;</span> <span class="pre">answer&quot;]</span></code>. Forgetting prepended spaces is a classic mistake when working with transformers!</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">AntonymDataset</span><span class="p">(</span><span class="n">WORD_PAIRS</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_prepended</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">corrupted</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">table</span> <span class="o">=</span> <span class="n">Table</span><span class="p">(</span><span class="s2">&quot;Prompt&quot;</span><span class="p">,</span> <span class="s2">&quot;Correct completion&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">seq</span><span class="p">,</span> <span class="n">completion</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">seqs</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">completions</span><span class="p">):</span>
    <span class="n">table</span><span class="o">.</span><span class="n">add_row</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">seq</span><span class="p">),</span> <span class="nb">repr</span><span class="p">(</span><span class="n">completion</span><span class="p">))</span>

<span class="n">rprint</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Prompt                                                   </span>┃<span style="font-weight: bold"> Correct completion </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ (comedy, tragedy), (harmony, discord), asleep -&gt;         │ ' awake'           │
│ (vertical, horizontal), (courage, cowardice), inflate -&gt; │ ' deflate'         │
│ (marry, divorce), (tenant, landlord), discord -&gt;         │ ' harmony'         │
│ (angel, devil), (compete, cooperate), giant -&gt;           │ ' dwarf'           │
│ (liberate, confine), (horizontal, vertical), human -&gt;    │ ' animal'          │
│ (noisy, silent), (before, after), benefit -&gt;             │ ' disadvantage'    │
│ (separate, join), (domestic, foreign), old -&gt;            │ ' young'           │
│ (unfaithful, faithful), (minimum, maximum), solid -&gt;     │ ' liquid'          │
│ (courage, cowardice), (bottom, top), honest -&gt;           │ ' deceitful'       │
│ (light, heavy), (deflate, inflate), empty -&gt;             │ ' full'            │
└──────────────────────────────────────────────────────────┴────────────────────┘
</pre></div>
</div>
<p>Compare this output to what it looks like when <code class="docutils literal notranslate"><span class="pre">corrupted=True</span></code>. You’ll see the second elements of each pair change to a random word, but the first elements (and the final pair) stay the same.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">AntonymDataset</span><span class="p">(</span><span class="n">WORD_PAIRS</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_prepended</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">corrupted</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">table</span> <span class="o">=</span> <span class="n">Table</span><span class="p">(</span><span class="s2">&quot;Prompt&quot;</span><span class="p">,</span> <span class="s2">&quot;Correct completion&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">seq</span><span class="p">,</span> <span class="n">completions</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">seqs</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">completions</span><span class="p">):</span>
    <span class="n">table</span><span class="o">.</span><span class="n">add_row</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">seq</span><span class="p">),</span> <span class="nb">repr</span><span class="p">(</span><span class="n">completions</span><span class="p">))</span>

<span class="n">rprint</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Prompt                                               </span>┃<span style="font-weight: bold"> Correct completion </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ (comedy, vertical), (harmony, faithful), asleep -&gt;   │ ' awake'           │
│ (vertical, after), (courage, angel), inflate -&gt;      │ ' deflate'         │
│ (marry, unconscious), (tenant, landlord), discord -&gt; │ ' harmony'         │
│ (angel, hard), (compete, departure), giant -&gt;        │ ' dwarf'           │
│ (liberate, below), (horizontal, exterior), human -&gt;  │ ' animal'          │
│ (noisy, knowledge), (before, comedy), benefit -&gt;     │ ' disadvantage'    │
│ (separate, empty), (domestic, wisdom), old -&gt;        │ ' young'           │
│ (unfaithful, interior), (minimum, win), solid -&gt;     │ ' liquid'          │
│ (courage, criminal), (bottom, slave), honest -&gt;      │ ' deceitful'       │
│ (light, folly), (deflate, guilty), empty -&gt;          │ ' full'            │
└──────────────────────────────────────────────────────┴────────────────────┘
</pre></div>
</div>
<details><summary><p>Aside - the rich library</p>
</summary><p>The <code class="docutils literal notranslate"><span class="pre">rich</span></code> library is a helpful little library to display outputs more clearly in a Python notebook or terminal. It’s not necessary for this workshop, but it’s a nice little tool to have in your toolbox.</p>
<p>The most important function is <code class="docutils literal notranslate"><span class="pre">rich.print</span></code> (usually imported as <code class="docutils literal notranslate"><span class="pre">rprint</span></code>). This can print basic strings, but it also supports the following syntax for printing colors:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">rprint</span><span class="p">(</span><span class="s2">&quot;[green]This is green text[/], this is default color&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="bef6cb9614cb4f7bba41acff11b866cf" class="no-scaled-link" src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/rprint-1.png" style="width: 350px;" /></p>
<p>and for making text bold / underlined:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">rprint</span><span class="p">(</span><span class="s2">&quot;[u dark_orange]This is underlined[/], and [b cyan]this is bold[/].&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="3ed85795c7ec466dafdff0ca0aa21b62" class="no-scaled-link" src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/rprint-2.png" style="width: 350px;" /></p>
<p>It can also print tables:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">rich.table</span> <span class="kn">import</span> <span class="n">Table</span>

<span class="n">table</span> <span class="o">=</span> <span class="n">Table</span><span class="p">(</span><span class="s2">&quot;Col1&quot;</span><span class="p">,</span> <span class="s2">&quot;Col2&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Title&quot;</span><span class="p">)</span> <span class="c1"># title is optional</span>
<span class="n">table</span><span class="o">.</span><span class="n">add_row</span><span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span>
<span class="n">table</span><span class="o">.</span><span class="n">add_row</span><span class="p">(</span><span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">)</span>

<span class="n">rprint</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="e3a96cb58af64d0bbdb7524497d03fed" class="no-scaled-link" src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/rprint-3.png" style="width: 150px;" /></p>
<p>The text formatting (bold, underlined, colors, etc) is also supported within table cells.</p>
</details><section id="Exercise---run-forward-pass-on-antonym-dataset">
<h3>Exercise - run forward pass on antonym dataset<a class="headerlink" href="#Exercise---run-forward-pass-on-antonym-dataset" title="Permalink to this heading">#</a></h3>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>Difficulty: 🔴🔴⚪⚪⚪
Importance: 🔵🔵🔵⚪⚪

You should spend up to 10-15 minutes on this exercise.
</pre></div>
</div>
<p>You should fill in the <code class="docutils literal notranslate"><span class="pre">calculate_h</span></code> function below. It should:</p>
<ul class="simple">
<li><p>Generate <code class="docutils literal notranslate"><span class="pre">N</span></code> random prompts from the antonym dataset (using the <code class="docutils literal notranslate"><span class="pre">create_prompts</span></code> method),</p></li>
<li><p>Run a forward pass on the model, using the <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> syntax we’ve demonstrated previously,</p></li>
<li><p>Return a tuple of the model’s output (i.e. its tokens) and the residual stream value at the end of layer <code class="docutils literal notranslate"><span class="pre">layer</span></code> (e.g. if <code class="docutils literal notranslate"><span class="pre">layer</span> <span class="pre">=</span> <span class="pre">-1</span></code>, this means the final value of the residual stream before we convert into logits).</p></li>
</ul>
<p><img alt="1f93f0cf8a784bd7816a7f1f9652ec21" class="no-scaled-link" src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/h-intervention-1.png" style="width: 900px;" /></p>
<p>You should only return the residual stream values for the very last sequence position in each prompt, i.e. the last <code class="docutils literal notranslate"><span class="pre">:</span></code> token (where the model makes the antonym prediction).</p>
<details><summary><p>Help - I’m not sure how to run (and index into) a batch of inputs.</p>
</summary><p>If we pass a list of strings to the <code class="docutils literal notranslate"><span class="pre">generator.invoke</span></code> function, this will be tokenized with padding automatically.</p>
<p>The type of padding which is applied is <strong>left padding</strong>, meaning if you index at sequence position <code class="docutils literal notranslate"><span class="pre">-1</span></code>, this will get the final token in the prompt for all prompts in the list, even if the prompts have different lengths.</p>
</details><div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calculate_h</span><span class="p">(</span><span class="n">dataset</span><span class="p">:</span> <span class="n">AntonymDataset</span><span class="p">,</span> <span class="n">layer</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Generates N random sequences of the form &quot;old:young, vanish:appear, dark:&quot;, but with 3 randomly chosen pairs (and orders).</span>

<span class="sd">    Averages over the hidden states of te last layer of GPT-J for each token in each sequence.</span>

<span class="sd">    Returns:</span>
<span class="sd">        completions: list of model completion strings (i.e. the strings the model predicts to follow the last token)</span>
<span class="sd">        h: average hidden state tensor at final sequence position, of shape (d_model,)</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">)</span> <span class="k">as</span> <span class="n">generator</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">prompts</span><span class="p">)</span> <span class="k">as</span> <span class="n">invoker</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

    <span class="n">completions</span> <span class="o">=</span> <span class="n">generator</span><span class="o">.</span><span class="n">output</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">completions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">completions</span><span class="p">)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">completions</span><span class="p">,</span> <span class="n">h</span>


<span class="n">tests</span><span class="o">.</span><span class="n">test_calculate_h</span><span class="p">(</span><span class="n">calculate_h</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
All tests in `test_calculate_h` passed.
</pre></div></div>
</div>
<p>We’ve provided you with a helper function, which displays the model’s output on the antonym dataset (and highlights the examples where the model’s prediction is correct).</p>
<p>If you’ve constructed your antonyms dataset well, you should find that the model’s completion is correct most of the time, and most of its mistakes are understandable (e.g. predicting <code class="docutils literal notranslate"><span class="pre">weak</span></code> rather than <code class="docutils literal notranslate"><span class="pre">fragile</span></code> as the antonym of <code class="docutils literal notranslate"><span class="pre">strong</span></code>). If we were being rigorous, we’d want to filter this dataset to make sure it only contains examples where the model can correctly perform the task - but here, we won’t worry about this.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">display_model_completions_on_antonyms</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">AntonymDataset</span><span class="p">,</span>
    <span class="n">completions</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">num_to_display</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">table</span> <span class="o">=</span> <span class="n">Table</span><span class="p">(</span><span class="s2">&quot;Prompt&quot;</span><span class="p">,</span> <span class="s2">&quot;Model&#39;s completion&quot;</span><span class="p">,</span> <span class="s2">&quot;Correct completion&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Model&#39;s antonym completions (green = first token is a match)&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">completions</span><span class="p">),</span> <span class="n">num_to_display</span><span class="p">)):</span>

        <span class="c1"># Get model&#39;s completion, and correct completion</span>
        <span class="n">completion</span> <span class="o">=</span> <span class="n">completions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">correct_completion</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">completions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">correct_completion_first_token</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">correct_completion</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;Ġ&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">)</span>
        <span class="n">seq</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">seqs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

        <span class="c1"># Color code the completion based on whether it&#39;s correct</span>
        <span class="n">is_correct</span> <span class="o">=</span> <span class="p">(</span><span class="n">completion</span> <span class="o">==</span> <span class="n">correct_completion_first_token</span><span class="p">)</span>
        <span class="n">completion</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;[b green]</span><span class="si">{</span><span class="nb">repr</span><span class="p">(</span><span class="n">completion</span><span class="p">)</span><span class="si">}</span><span class="s2">[/]&quot;</span> <span class="k">if</span> <span class="n">is_correct</span> <span class="k">else</span> <span class="nb">repr</span><span class="p">(</span><span class="n">completion</span><span class="p">)</span>

        <span class="n">table</span><span class="o">.</span><span class="n">add_row</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">seq</span><span class="p">),</span> <span class="n">completion</span><span class="p">,</span> <span class="nb">repr</span><span class="p">(</span><span class="n">correct_completion</span><span class="p">))</span>

    <span class="n">rprint</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get uncorrupted dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">AntonymDataset</span><span class="p">(</span><span class="n">WORD_PAIRS</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_prepended</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Getting it from layer 12, cause the graph suggested this was where there was high accuracy</span>
<span class="n">model_completions</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">calculate_h</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>

<span class="c1"># Displaying the output</span>
<span class="n">display_model_completions_on_antonyms</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">model_completions</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-style: italic">                  Model's antonym completions (green = first token is a match)                   </span>
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Prompt                                              </span>┃<span style="font-weight: bold"> Model's completion </span>┃<span style="font-weight: bold"> Correct completion </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ (awake, asleep), (sad, happy), pass -&gt;              │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' fail'</span>            │ ' fail'            │
│ (first, last), (healthy, sick), major -&gt;            │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' minor'</span>           │ ' minor'           │
│ (increase, decrease), (slavery, freedom), closed -&gt; │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' open'</span>            │ ' open'            │
│ (fantasy, reality), (domestic, wild), brave -&gt;      │ ' afraid'          │ ' cowardly'        │
│ (loose, tight), (careful, careless), calm -&gt;        │ ' calm'            │ ' stormy'          │
│ (first, last), (exterior, interior), south -&gt;       │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' north'</span>           │ ' north'           │
│ (paradise, hell), (courage, fear), deep -&gt;          │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' shallow'</span>         │ ' shallow'         │
│ (courage, fear), (include, exclude), up -&gt;          │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' down'</span>            │ ' down'            │
│ (future, past), (bottom, top), thin -&gt;              │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' thick'</span>           │ ' thick'           │
│ (worst, best), (maximum, minimum), fast -&gt;          │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' slow'</span>            │ ' slow'            │
│ (hot, cold), (reject, accept), past -&gt;              │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' future'</span>          │ ' future'          │
│ (reject, accept), (modern, ancient), dark -&gt;        │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' light'</span>           │ ' light'           │
│ (sane, insane), (exciting, boring), careful -&gt;      │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' careless'</span>        │ ' careless'        │
│ (rough, gentle), (buy, sell), insane -&gt;             │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' sane'</span>            │ ' sane'            │
│ (moral, immoral), (fear, courage), humble -&gt;        │ ' proud'           │ ' arrogant'        │
│ (arrive, depart), (young, old), join -&gt;             │ ' leave'           │ ' separate'        │
│ (low, high), (rough, gentle), immoral -&gt;            │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' moral'</span>           │ ' moral'           │
│ (stupid, smart), (combine, separate), brave -&gt;      │ ' stupid'          │ ' cowardly'        │
│ (courage, fear), (junior, senior), shallow -&gt;       │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' deep'</span>            │ ' deep'            │
│ (inferior, superior), (exclude, include), yes -&gt;    │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' no'</span>              │ ' no'              │
└─────────────────────────────────────────────────────┴────────────────────┴────────────────────┘
</pre></div>
</div>
</section>
<section id="Exercise---intervene-with-h">
<h3>Exercise - intervene with <span class="math notranslate nohighlight">\(h\)</span><a class="headerlink" href="#Exercise---intervene-with-h" title="Permalink to this heading">#</a></h3>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>Difficulty: 🔴🔴🔴⚪⚪
Importance: 🔵🔵🔵🔵⚪

You should spend up to 10-15 minutes on this exercise.
</pre></div>
</div>
<p>You should fill in the function <code class="docutils literal notranslate"><span class="pre">intervene_with_h</span></code> below. This will involve:</p>
<ul class="simple">
<li><p>Using the <code class="docutils literal notranslate"><span class="pre">calculate_h</span></code> function you just wrote to get the h-vector (this code is already filled in below),</p></li>
<li><p>Defining a zero-shot dataset, i.e. with no prepended antonym pairs,</p></li>
<li><p>Run two forward passes (within the same context manager):</p>
<ul>
<li><p>One with no intervention (i.e. <code class="docutils literal notranslate"><span class="pre">h</span></code> is unchanged),</p></li>
<li><p>One with an intervention on <code class="docutils literal notranslate"><span class="pre">h</span></code> (i.e. the residual stream value is set to <code class="docutils literal notranslate"><span class="pre">h</span></code>, at the layer which <code class="docutils literal notranslate"><span class="pre">h</span></code> was taken from).</p></li>
</ul>
</li>
<li><p>Return the zero-shot dataset, as well as the completions for no intervention and intervention cases respectively (see docstring).</p></li>
</ul>
<p>The diagram below shows how all of this should work, when combined with the <code class="docutils literal notranslate"><span class="pre">calculate_h</span></code> function.</p>
<p><img alt="2d0619e0cbb942bbbeea5c58319a81b7" class="no-scaled-link" src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/h-intervention-2.png" style="width: 950px;" /></p>
<p>Hint - you can use <code class="docutils literal notranslate"><span class="pre">tokenizer.batch_decode</span></code> to turn a list of tokens into a list of strings.</p>
<details><summary><p>Help - I’m not sure how best to get both the no-intervention and intervention completions.</p>
</summary><p>You can use <code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">generator.invoke...</span></code> more than once within the same context manager, in order to add to your batch. This will eventually give you output of shape (2*N, seq_len), which can then be indexed and reshaped to get the completions in the no intervention &amp; intervention cases respectively.</p>
</details><details><summary><p>Help - I’m not sure how to intervene on the hidden state.</p>
</summary><p>First, you can define the tensor of hidden states (i.e. using <code class="docutils literal notranslate"><span class="pre">.output[0]</span></code>, like you’ve done before).</p>
<p>Then, you can add to this tensor directly (or add to some indexed version of it). You can use inplace operations (i.e. <code class="docutils literal notranslate"><span class="pre">tensor</span> <span class="pre">+=</span> <span class="pre">h</span></code>) or redefining the tensor (i.e. <code class="docutils literal notranslate"><span class="pre">tensor</span> <span class="pre">=</span> <span class="pre">tensor</span> <span class="pre">+</span> <span class="pre">h</span></code>); either work.</p>
<p>You won’t need to <code class="docutils literal notranslate"><span class="pre">.save()</span></code> anything here; we’re just intervening rather than storing the value of the residual stream.</p>
</details><div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">intervene_with_h</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">AntonymDataset</span><span class="p">,</span>
    <span class="n">layer</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">zero_shot_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">AntonymDataset</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Extracts the vector `h` using previously defined function, and intervenes by adding `h` to the</span>
<span class="sd">    residual stream of a set of generated zero-shot prompts.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        word_list: the list of words used to create the prompts</span>
<span class="sd">        dataset: the dataset of clean prompts from which we&#39;ll extract the `h`-vector</span>
<span class="sd">        layer: the layer we&#39;ll be extracting the `h`-vector from</span>
<span class="sd">        zero_shot_size: the number of zero-shot prompts to generate, to test our intervention</span>

<span class="sd">    Returns:</span>
<span class="sd">        zero_shot_dataset: the dataset of zero-shot prompts, which you should generate in this fn</span>
<span class="sd">        completions: list of string completions for the zero-shot prompts, without intervention</span>
<span class="sd">        completions_intervention: list of string completions for the zero-shot prompts, with h-intervention</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># Run previous function to get h-vector</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">calculate_h</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Get zero-shot dataset</span>
    <span class="n">zero_shot_dataset</span> <span class="o">=</span> <span class="n">AntonymDataset</span><span class="p">(</span><span class="n">WORD_PAIRS</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">zero_shot_size</span><span class="p">,</span> <span class="n">n_prepended</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">)</span> <span class="k">as</span> <span class="n">generator</span><span class="p">:</span>

        <span class="c1"># First, run a forward pass where we don&#39;t intervene</span>
        <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">zero_shot_dataset</span><span class="o">.</span><span class="n">prompts</span><span class="p">)</span> <span class="k">as</span> <span class="n">invoker</span><span class="p">:</span>
            <span class="k">pass</span>

        <span class="c1"># Next, run a forward pass on the zero-shot prompts where we do intervene</span>
        <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">zero_shot_dataset</span><span class="o">.</span><span class="n">prompts</span><span class="p">)</span> <span class="k">as</span> <span class="n">invoker</span><span class="p">:</span>
            <span class="c1"># Access the tensor (which is the first element of the output tuple)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="c1"># Add the h-vector to the residual stream, at the last sequence position</span>
            <span class="n">hidden_states</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">h</span>

    <span class="c1"># Get the output (token IDs), reshape it into 2 rows of (no intervention, intervention)</span>
    <span class="n">token_completions</span> <span class="o">=</span> <span class="n">generator</span><span class="o">.</span><span class="n">output</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">zero_shot_size</span><span class="p">)</span>
    <span class="c1"># Decode to get the string tokens</span>
    <span class="n">completions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">token_completions</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">completions_intervention</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">token_completions</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">zero_shot_dataset</span><span class="p">,</span> <span class="n">completions</span><span class="p">,</span> <span class="n">completions_intervention</span>
</pre></div>
</div>
</div>
<p>Next, you can run the code below to see the results of your intervention. In cases where your model is correct, its completion is highlighted in green.</p>
<p>(Note, we’re using the <code class="docutils literal notranslate"><span class="pre">repr</span></code> function, because a lot of the completions are line breaks, and this helps us see them more clearly!)</p>
<p>If you’ve done this correctly, you should see at least a few correct completions (~25%).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">display_model_completions_on_h_intervention</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">AntonymDataset</span><span class="p">,</span>
    <span class="n">completions</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">completions_intervention</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">num_to_display</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">table</span> <span class="o">=</span> <span class="n">Table</span><span class="p">(</span>
        <span class="s2">&quot;Prompt&quot;</span><span class="p">,</span> <span class="s2">&quot;Model&#39;s completion</span><span class="se">\n</span><span class="s2">(no intervention)&quot;</span><span class="p">,</span> <span class="s2">&quot;Model&#39;s completion</span><span class="se">\n</span><span class="s2">(intervention)&quot;</span><span class="p">,</span> <span class="s2">&quot;Correct completion&quot;</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Model&#39;s antonym completions</span><span class="se">\n</span><span class="s2">(green = first token is a match)&quot;</span>
    <span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">completions</span><span class="p">),</span> <span class="n">num_to_display</span><span class="p">)):</span>

        <span class="n">completion_ni</span> <span class="o">=</span> <span class="n">completions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">completion_i</span> <span class="o">=</span> <span class="n">completions_intervention</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">correct_completion</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">completions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">correct_completion_first_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">correct_completion</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;Ġ&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">)</span>
        <span class="n">seq</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">seqs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

        <span class="c1"># Color code the completion based on whether it&#39;s correct</span>
        <span class="n">is_correct</span> <span class="o">=</span> <span class="p">(</span><span class="n">completion_i</span> <span class="o">==</span> <span class="n">correct_completion_first_token</span><span class="p">)</span>
        <span class="n">completion_i</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;[b green]</span><span class="si">{</span><span class="nb">repr</span><span class="p">(</span><span class="n">completion_i</span><span class="p">)</span><span class="si">}</span><span class="s2">[/]&quot;</span> <span class="k">if</span> <span class="n">is_correct</span> <span class="k">else</span> <span class="nb">repr</span><span class="p">(</span><span class="n">completion_i</span><span class="p">)</span>

        <span class="n">table</span><span class="o">.</span><span class="n">add_row</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">seq</span><span class="p">),</span> <span class="nb">repr</span><span class="p">(</span><span class="n">completion_ni</span><span class="p">),</span> <span class="n">completion_i</span><span class="p">,</span> <span class="nb">repr</span><span class="p">(</span><span class="n">correct_completion</span><span class="p">))</span>

    <span class="n">rprint</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">AntonymDataset</span><span class="p">(</span><span class="n">WORD_PAIRS</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_prepended</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">zero_shot_dataset</span><span class="p">,</span> <span class="n">model_completions</span><span class="p">,</span> <span class="n">model_completions_intervention</span> <span class="o">=</span> <span class="n">intervene_with_h</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">zero_shot_size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">display_model_completions_on_h_intervention</span><span class="p">(</span><span class="n">zero_shot_dataset</span><span class="p">,</span> <span class="n">model_completions</span><span class="p">,</span> <span class="n">model_completions_intervention</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-style: italic">         Model's antonym completions (green = first token is a match)         </span>
┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold">             </span>┃<span style="font-weight: bold"> Model's completion </span>┃<span style="font-weight: bold"> Model's completion </span>┃<span style="font-weight: bold">                    </span>┃
┃<span style="font-weight: bold"> Prompt      </span>┃<span style="font-weight: bold"> (no intervention)  </span>┃<span style="font-weight: bold"> (intervention)     </span>┃<span style="font-weight: bold"> Correct completion </span>┃
┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ awake -&gt;    │ ' awake'           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' asleep'</span>          │ ' asleep'          │
│ first -&gt;    │ ' second'          │ ' second'          │ ' last'            │
│ increase -&gt; │ ' increase'        │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' decrease'</span>        │ ' decrease'        │
│ fantasy -&gt;  │ ' yes'             │ ' fantasy'         │ ' reality'         │
│ loose -&gt;    │ ' loose'           │ ' loose'           │ ' tight'           │
│ first -&gt;    │ ' second'          │ ' second'          │ ' last'            │
│ paradise -&gt; │ ' paradise'        │ ' paradise'        │ ' hell'            │
│ courage -&gt;  │ ' courage'         │ ' courage'         │ ' fear'            │
│ future -&gt;   │ ' future'          │ ' future'          │ ' past'            │
│ worst -&gt;    │ ' worst'           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' best'</span>            │ ' best'            │
│ hot -&gt;      │ ' hot'             │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' cold'</span>            │ ' cold'            │
│ reject -&gt;   │ ' reject'          │ ' reject'          │ ' accept'          │
│ sane -&gt;     │ ' yes'             │ ' sane'            │ ' insane'          │
│ rough -&gt;    │ ' rough'           │ ' rough'           │ ' gentle'          │
│ moral -&gt;    │ ' moral'           │ ' moral'           │ ' immoral'         │
│ arrive -&gt;   │ ' arrive'          │ ' arrive'          │ ' depart'          │
│ low -&gt;      │ ' low'             │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' high'</span>            │ ' high'            │
│ stupid -&gt;   │ ' stupid'          │ ' stupid'          │ ' smart'           │
│ courage -&gt;  │ ' courage'         │ ' courage'         │ ' fear'            │
│ inferior -&gt; │ ' superior'        │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' superior'</span>        │ ' superior'        │
└─────────────┴────────────────────┴────────────────────┴────────────────────┘
</pre></div>
</div>
</section>
<section id="Exercise---combine-the-functions-calculate_h-and-intervene_with_h">
<h3>Exercise - combine the functions <code class="docutils literal notranslate"><span class="pre">calculate_h</span></code> and <code class="docutils literal notranslate"><span class="pre">intervene_with_h</span></code><a class="headerlink" href="#Exercise---combine-the-functions-calculate_h-and-intervene_with_h" title="Permalink to this heading">#</a></h3>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>Difficulty: 🔴🔴🔴⚪⚪
Importance: 🔵🔵🔵⚪⚪

You should spend up to 10-15 minutes on this exercise.
</pre></div>
</div>
<p>One great feature of the <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> library is its ability to parallelize forward passes and perform complex interventions within a single context manager.</p>
<p>In the code above, we had one function to extract the hidden states from the model, and another function where we intervened with those hidden states. But we can actually do both at once: we can compute <span class="math notranslate nohighlight">\(h\)</span> within our forward pass, and then intervene with it on a different forward pass (using our zero-shot prompts), all within the same <code class="docutils literal notranslate"><span class="pre">model.generate</span></code> context manager. In other words, <strong>we’ll be using ``with generator.invoke…`` three times</strong> in this context manager.</p>
<p><img alt="aa394bc676ad44d99934a634c3bd064f" class="no-scaled-link" src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/h-intervention-3.png" style="width: 1000px;" /></p>
<p>You should fill in the <code class="docutils literal notranslate"><span class="pre">calculate_h_and_intervene</span></code> function below, to do this. Mostly, this should involve combining your <code class="docutils literal notranslate"><span class="pre">calculate_h</span></code> and <code class="docutils literal notranslate"><span class="pre">intervene_with_h</span></code> functions, and wrapping the forward passes in the same context manager (plus a bit of code rewriting).</p>
<p>Your output should be exactly the same as before (since the <code class="docutils literal notranslate"><span class="pre">AntonymDataset</span></code> class is deterministic).</p>
<details><summary><p>Help - I’m not sure how to work with the h vector.</p>
</summary><p>You extract <code class="docutils literal notranslate"><span class="pre">h</span></code> the same way as before, but you don’t need to save it, or ever reference its <code class="docutils literal notranslate"><span class="pre">.value</span></code> attribute. It is kept as a proxy. You can still use it later in the context manager, just like it actually was a tensor.</p>
<p>You shouldn’t have to <code class="docutils literal notranslate"><span class="pre">.save()</span></code> anything inside your context manager.</p>
</details><div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calculate_h_and_intervene</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">AntonymDataset</span><span class="p">,</span>
    <span class="n">layer</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">zero_shot_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">AntonymDataset</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Extracts the vector `h`, intervenes by adding `h` to the residual stream of a set of generated zero-shot prompts,</span>
<span class="sd">    all within the same forward pass.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        word_list: the list of words used to create the prompts</span>
<span class="sd">        dataset: the dataset of clean prompts from which we&#39;ll extract the `h`-vector</span>
<span class="sd">        layer: the layer we&#39;ll be extracting the `h`-vector from</span>
<span class="sd">        zero_shot_size: the number of zero-shot prompts to generate, to test our intervention</span>

<span class="sd">    Returns:</span>
<span class="sd">        zero_shot_dataset: the dataset of zero-shot prompts, which you should generate in this fn</span>
<span class="sd">        model_completions: list of string completions for the zero-shot prompts, without intervention</span>
<span class="sd">        model_completions_intervention: list of string completions for the zero-shot prompts, with h-intervention</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="c1"># Get zero-shot dataset</span>
    <span class="n">zero_shot_dataset</span> <span class="o">=</span> <span class="n">AntonymDataset</span><span class="p">(</span><span class="n">WORD_PAIRS</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">zero_shot_size</span><span class="p">,</span> <span class="n">n_prepended</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">as</span> <span class="n">generator</span><span class="p">:</span>

        <span class="c1"># Run on the clean prompts, to get the h-vector</span>
        <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">prompts</span><span class="p">)</span> <span class="k">as</span> <span class="n">invoker</span><span class="p">:</span>
            <span class="c1"># Define h (we don&#39;t need to save it, cause we don&#39;t need it outside `generator:`)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># First, run a forward pass where we don&#39;t intervene</span>
        <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">zero_shot_dataset</span><span class="o">.</span><span class="n">prompts</span><span class="p">)</span> <span class="k">as</span> <span class="n">invoker</span><span class="p">:</span>
            <span class="k">pass</span>

        <span class="c1"># Next, run a forward pass on the zero-shot prompts where we do intervene</span>
        <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">zero_shot_dataset</span><span class="o">.</span><span class="n">prompts</span><span class="p">)</span> <span class="k">as</span> <span class="n">invoker</span><span class="p">:</span>
            <span class="c1"># Access the tensor (which is the first element of the output tuple)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="c1"># Add the h-vector to the residual stream, at the last sequence position</span>
            <span class="n">hidden_states</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">h</span>

    <span class="c1"># Get the output (token IDs), keep data from zero-shot dataset reshape into (no intervention, intervention)</span>
    <span class="n">token_completions</span> <span class="o">=</span> <span class="n">generator</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">):,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">zero_shot_size</span><span class="p">)</span>
    <span class="c1"># Decode to get the string tokens</span>
    <span class="n">model_completions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">token_completions</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">model_completions_intervention</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">token_completions</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">zero_shot_dataset</span><span class="p">,</span> <span class="n">model_completions</span><span class="p">,</span> <span class="n">model_completions_intervention</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">AntonymDataset</span><span class="p">(</span><span class="n">WORD_PAIRS</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_prepended</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">zero_shot_dataset</span><span class="p">,</span> <span class="n">model_completions</span><span class="p">,</span> <span class="n">model_completions_intervention</span> <span class="o">=</span> <span class="n">calculate_h_and_intervene</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">zero_shot_size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">display_model_completions_on_h_intervention</span><span class="p">(</span><span class="n">zero_shot_dataset</span><span class="p">,</span> <span class="n">model_completions</span><span class="p">,</span> <span class="n">model_completions_intervention</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-style: italic">         Model's antonym completions (green = first token is a match)         </span>
┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold">             </span>┃<span style="font-weight: bold"> Model's completion </span>┃<span style="font-weight: bold"> Model's completion </span>┃<span style="font-weight: bold">                    </span>┃
┃<span style="font-weight: bold"> Prompt      </span>┃<span style="font-weight: bold"> (no intervention)  </span>┃<span style="font-weight: bold"> (intervention)     </span>┃<span style="font-weight: bold"> Correct completion </span>┃
┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ awake -&gt;    │ ' awake'           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' asleep'</span>          │ ' asleep'          │
│ first -&gt;    │ ' second'          │ ' second'          │ ' last'            │
│ increase -&gt; │ ' increase'        │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' decrease'</span>        │ ' decrease'        │
│ fantasy -&gt;  │ ' yes'             │ ' fantasy'         │ ' reality'         │
│ loose -&gt;    │ ' loose'           │ ' loose'           │ ' tight'           │
│ first -&gt;    │ ' second'          │ ' second'          │ ' last'            │
│ paradise -&gt; │ ' paradise'        │ ' paradise'        │ ' hell'            │
│ courage -&gt;  │ ' courage'         │ ' courage'         │ ' fear'            │
│ future -&gt;   │ ' future'          │ ' future'          │ ' past'            │
│ worst -&gt;    │ ' worst'           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' best'</span>            │ ' best'            │
│ hot -&gt;      │ ' hot'             │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' cold'</span>            │ ' cold'            │
│ reject -&gt;   │ ' reject'          │ ' reject'          │ ' accept'          │
│ sane -&gt;     │ ' yes'             │ ' sane'            │ ' insane'          │
│ rough -&gt;    │ ' rough'           │ ' rough'           │ ' gentle'          │
│ moral -&gt;    │ ' moral'           │ ' moral'           │ ' immoral'         │
│ arrive -&gt;   │ ' arrive'          │ ' arrive'          │ ' depart'          │
│ low -&gt;      │ ' low'             │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' high'</span>            │ ' high'            │
│ stupid -&gt;   │ ' stupid'          │ ' stupid'          │ ' smart'           │
│ courage -&gt;  │ ' courage'         │ ' courage'         │ ' fear'            │
│ inferior -&gt; │ ' superior'        │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' superior'</span>        │ ' superior'        │
└─────────────┴────────────────────┴────────────────────┴────────────────────┘
</pre></div>
</div>
</section>
</section>
<section id="Logit-outputs">
<h2>Logit outputs<a class="headerlink" href="#Logit-outputs" title="Permalink to this heading">#</a></h2>
<p>Currently, we’ve only seen what the model’s highest-probability output is, because that’s all we got from the <code class="docutils literal notranslate"><span class="pre">generator.output</span></code> object. But what if we want to look at the logits / probabilities instead, and see how those change when we intervene?</p>
<p>There are two ways you can get the model’s logits:</p>
<ol class="arabic simple">
<li><p><strong>Add more arguments to the ``generate`` method.</strong></p></li>
</ol>
<p>Just like standard HuggingFace models have extra arguments which can be supplied to the <code class="docutils literal notranslate"><span class="pre">generate</span></code> method, so do <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> models. We can replace the line:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">...</span><span class="p">)</span> <span class="k">as</span> <span class="n">generator</span><span class="p">:</span>
</pre></div>
</div>
<p>with:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">output_scores</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">generator</span><span class="p">:</span>
</pre></div>
</div>
<p>and then the <code class="docutils literal notranslate"><span class="pre">generator.output</span></code> object won’t be a tensor containing the model’s completion, instead it will be an object that contains both the completions <strong>and</strong> the logits. From this object, you can access:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">generator.output.sequences</span></code> = model completions, i.e. a tensor of token IDs, of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">seq_len)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">generator.output.scores</span></code> = logits, in the form of a tuple of tensors of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">seq_len,</span> <span class="pre">vocab_size)</span></code> (the tuple has one element for each token generation, so in this case it will be length 1).</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>For general models, you can access the logits just like you would any other hidden state.</strong></p></li>
</ol>
<p>For example, in the case of GPT-J and other similar models, you can access the output of the final linear layer of the transformer - i.e. the one that maps from the hidden state to logits - with <code class="docutils literal notranslate"><span class="pre">model.lm_head</span></code>. You can then use the <code class="docutils literal notranslate"><span class="pre">.output</span></code> method to get a proxy for the output of this layer, and <code class="docutils literal notranslate"><span class="pre">.save()</span></code> to save it, just like you’ve done in previous exercises.</p>
<p>Either of these approaches are fine, which one you use is up to personal preference. The solutions will use the first approach.</p>
<section id="Exercise---compute-change-in-accuracy">
<h3>Exercise - compute change in accuracy<a class="headerlink" href="#Exercise---compute-change-in-accuracy" title="Permalink to this heading">#</a></h3>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>Difficulty: 🔴🔴⚪⚪⚪
Importance: 🔵🔵🔵⚪⚪

You should spend up to 10-20 minutes on this exercise.
</pre></div>
</div>
<p>You should now rewrite the <code class="docutils literal notranslate"><span class="pre">calculate_h_and_intervene</span></code> function so that, rather than returning the string completions, it returns two lists of floats, containing the <strong>logprobs assigned by the model to the correct antonym</strong> in the no intervention / intervention cases respectively.</p>
<p>When you run the code below this function, it will display the log-probabilities (highlighting green when they increase from the zero-shot case). You should find that in every sequence, the logprobs on the correct token increase in the intervention. This helps make something clear - <strong>even if the maximum-likelihood token doesn’t change, this doesn’t mean that the intervention isn’t having a significant effect.</strong></p>
<details><summary><p>Help - I don’t know how to get the correct logprobs from the logits.</p>
</summary><p>First, apply log softmax to the logits, to get logprobs.</p>
<p>Second, you can use <code class="docutils literal notranslate"><span class="pre">tokenizer(dataset.completion)[&quot;input_ids&quot;]</span></code> to get the token IDs of the correct completions. (Gotcha - some words might be tokenized into multiple tokens, so make sure you’re just picking the first token ID for each completion.)</p>
</details><div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calculate_h_and_intervene_logprobs</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">AntonymDataset</span><span class="p">,</span>
    <span class="n">layer</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">zero_shot_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">AntonymDataset</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Extracts the vector `h`, intervenes by adding `h` to the residual stream of a set of generated zero-shot prompts,</span>
<span class="sd">    all within the same forward pass.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        word_list: the list of words used to create the prompts</span>
<span class="sd">        dataset: the dataset of clean prompts from which we&#39;ll extract the `h`-vector</span>
<span class="sd">        layer: the layer we&#39;ll be extracting the `h`-vector from</span>
<span class="sd">        zero_shot_size: the number of zero-shot prompts to generate, to test our intervention</span>

<span class="sd">    Returns:</span>
<span class="sd">        zero_shot_dataset: the dataset of zero-shot prompts, which you should generate in this fn</span>
<span class="sd">        correct_logprobs: list of correct-token logprobs for the zero-shot prompts, without intervention</span>
<span class="sd">        correct_logprobs_intervention: list of correct-token logprobs for the zero-shot prompts, with h-intervention</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="c1"># Get zero-shot dataset</span>
    <span class="n">zero_shot_dataset</span> <span class="o">=</span> <span class="n">AntonymDataset</span><span class="p">(</span><span class="n">WORD_PAIRS</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">zero_shot_size</span><span class="p">,</span> <span class="n">n_prepended</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="n">output_scores</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">generator</span><span class="p">:</span>

        <span class="c1"># Clean prompts, to get the h-vector</span>
        <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">prompts</span><span class="p">)</span> <span class="k">as</span> <span class="n">invoker</span><span class="p">:</span>
            <span class="c1"># Define h (we don&#39;t need to save it, cause we don&#39;t need it outside `generator:`)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Zero-shot prompts, no intervention</span>
        <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">zero_shot_dataset</span><span class="o">.</span><span class="n">prompts</span><span class="p">)</span> <span class="k">as</span> <span class="n">invoker</span><span class="p">:</span>
            <span class="k">pass</span>

        <span class="c1"># Zero-shot prompts, intervention with h</span>
        <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">zero_shot_dataset</span><span class="o">.</span><span class="n">prompts</span><span class="p">)</span> <span class="k">as</span> <span class="n">invoker</span><span class="p">:</span>
            <span class="c1"># Access the tensor (which is the first element of the output tuple)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="c1"># Add the h-vector to the residual stream, at the last sequence position</span>
            <span class="n">hidden_states</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">h</span>

    <span class="c1"># Get logits, slice to remove the `dataset` outputs, and reshape into (2, zero_shot_size, d_vocab)</span>
    <span class="n">logits</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">generator</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">scores</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">):]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">zero_shot_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">logprobs</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Get correct completions from `dataset`, and use these to index into the logprobs</span>
    <span class="n">correct_completion_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">toks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">toks</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">zero_shot_dataset</span><span class="o">.</span><span class="n">completions</span><span class="p">)[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]]</span>
    <span class="n">correct_logprobs</span><span class="p">,</span> <span class="n">correct_logprobs_intervention</span> <span class="o">=</span> <span class="n">logprobs</span><span class="p">[:,</span> <span class="nb">range</span><span class="p">(</span><span class="n">zero_shot_size</span><span class="p">),</span> <span class="n">correct_completion_ids</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">zero_shot_dataset</span><span class="p">,</span> <span class="n">correct_logprobs</span><span class="p">,</span> <span class="n">correct_logprobs_intervention</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">display_model_logprobs_on_h_intervention</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">AntonymDataset</span><span class="p">,</span>
    <span class="n">correct_logprobs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
    <span class="n">correct_logprobs_intervention</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
    <span class="n">num_to_display</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">table</span> <span class="o">=</span> <span class="n">Table</span><span class="p">(</span>
        <span class="s2">&quot;Zero-shot prompt&quot;</span><span class="p">,</span> <span class="s2">&quot;Model&#39;s logprob</span><span class="se">\n</span><span class="s2">(no intervention)&quot;</span><span class="p">,</span> <span class="s2">&quot;Model&#39;s logprob</span><span class="se">\n</span><span class="s2">(intervention)&quot;</span><span class="p">,</span> <span class="s2">&quot;Change in logprob&quot;</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Model&#39;s antonym logprobs, with zero-shot h-intervention</span><span class="se">\n</span><span class="s2">(green = intervention improves accuracy)&quot;</span>
    <span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">correct_logprobs</span><span class="p">),</span> <span class="n">num_to_display</span><span class="p">)):</span>

        <span class="n">logprob_ni</span> <span class="o">=</span> <span class="n">correct_logprobs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">logprob_i</span> <span class="o">=</span> <span class="n">correct_logprobs_intervention</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">delta_logprob</span> <span class="o">=</span> <span class="n">logprob_i</span> <span class="o">-</span> <span class="n">logprob_ni</span>
        <span class="n">zero_shot_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">dataset</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;8</span><span class="si">}</span><span class="s2"> -&gt; </span><span class="si">{</span><span class="n">dataset</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>

        <span class="c1"># Color code the logprob based on whether it&#39;s increased with this intervention</span>
        <span class="n">is_improvement</span> <span class="o">=</span> <span class="p">(</span><span class="n">delta_logprob</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">delta_logprob</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;[b green]</span><span class="si">{</span><span class="n">delta_logprob</span><span class="si">:</span><span class="s2">+.2f</span><span class="si">}</span><span class="s2">[/]&quot;</span> <span class="k">if</span> <span class="n">is_improvement</span> <span class="k">else</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">delta_logprob</span><span class="si">:</span><span class="s2">+.2f</span><span class="si">}</span><span class="s2">&quot;</span>

        <span class="n">table</span><span class="o">.</span><span class="n">add_row</span><span class="p">(</span><span class="n">zero_shot_prompt</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">logprob_ni</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">logprob_i</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">delta_logprob</span><span class="p">)</span>

    <span class="n">rprint</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">AntonymDataset</span><span class="p">(</span><span class="n">WORD_PAIRS</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_prepended</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">zero_shot_dataset</span><span class="p">,</span> <span class="n">correct_logprobs</span><span class="p">,</span> <span class="n">correct_logprobs_intervention</span> <span class="o">=</span> <span class="n">calculate_h_and_intervene_logprobs</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">zero_shot_size</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>

<span class="n">display_model_logprobs_on_h_intervention</span><span class="p">(</span><span class="n">zero_shot_dataset</span><span class="p">,</span> <span class="n">correct_logprobs</span><span class="p">,</span> <span class="n">correct_logprobs_intervention</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-style: italic">             Model's antonym logprobs, with zero-shot h-intervention              </span>
<span style="font-style: italic">                     (green = intervention improves accuracy)                     </span>
┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold">                      </span>┃<span style="font-weight: bold"> Model's logprob   </span>┃<span style="font-weight: bold"> Model's logprob </span>┃<span style="font-weight: bold">                   </span>┃
┃<span style="font-weight: bold"> Zero-shot prompt     </span>┃<span style="font-weight: bold"> (no intervention) </span>┃<span style="font-weight: bold"> (intervention)  </span>┃<span style="font-weight: bold"> Change in logprob </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│    awake -&gt; asleep   │ -3.48             │ -1.83           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.65</span>             │
│    first -&gt; last     │ -4.49             │ -3.04           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.45</span>             │
│ increase -&gt; decrease │ -3.28             │ -0.97           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+2.30</span>             │
│  fantasy -&gt; reality  │ -7.87             │ -4.71           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+3.16</span>             │
│    loose -&gt; tight    │ -5.53             │ -3.20           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+2.33</span>             │
│    first -&gt; last     │ -4.49             │ -3.04           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.45</span>             │
│ paradise -&gt; hell     │ -4.99             │ -3.27           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.72</span>             │
│  courage -&gt; fear     │ -5.21             │ -3.48           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.73</span>             │
│   future -&gt; past     │ -3.82             │ -2.62           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.20</span>             │
│    worst -&gt; best     │ -2.07             │ -1.34           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+0.73</span>             │
│      hot -&gt; cold     │ -2.18             │ -0.60           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.58</span>             │
│   reject -&gt; accept   │ -3.92             │ -2.20           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.72</span>             │
│     sane -&gt; insane   │ -7.13             │ -4.45           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+2.68</span>             │
│    rough -&gt; gentle   │ -7.49             │ -5.66           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.82</span>             │
│    moral -&gt; immoral  │ -4.61             │ -2.98           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.63</span>             │
│   arrive -&gt; depart   │ -5.66             │ -4.06           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.60</span>             │
│      low -&gt; high     │ -1.96             │ -0.80           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.16</span>             │
│   stupid -&gt; smart    │ -5.35             │ -3.52           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.83</span>             │
│  courage -&gt; fear     │ -5.21             │ -3.48           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.73</span>             │
│ inferior -&gt; superior │ -1.88             │ -1.02           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+0.87</span>             │
└──────────────────────┴───────────────────┴─────────────────┴───────────────────┘
</pre></div>
</div>
</section>
</section>
</section>
<section id="id3">
<h1>3️⃣ Function Vectors<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h1>
<p>In this section, we’ll move from thinking about residual stream states to thinking about the <strong>output of specific attention heads.</strong></p>
<p>First, a bit of a technical complication. Most HuggingFace models don’t have the nice attention head representations that TransformerLens models do (i.e. storing vectors &amp; attention weights separately by heads). In the case of GPT-J, the input to <code class="docutils literal notranslate"><span class="pre">out_proj</span></code> (the final linear map in the attention layer) is a tensor of value vectors which has already been concatenated along attention heads, and applying <code class="docutils literal notranslate"><span class="pre">out_proj</span></code> is equivalent to summing over the attention heads (if you can’t see how this is
possible, see the section “Attention Heads are Independent and Additive” from Anthropic’s <a class="reference external" href="https://transformer-circuits.pub/2021/framework/index.html">Mathematical Framework</a>).</p>
<p>How can we deal with this? The easiest way is to just intervene on the input of <code class="docutils literal notranslate"><span class="pre">out_proj</span></code> instead (since this is causally the same as intervening on the output), and making sure we reshape this input tensor so that it has a head dimension (then we can intervene more easily on a per-head basis). In other words, you should intervene on the value which we’ve called <code class="docutils literal notranslate"><span class="pre">z</span></code> in the diagram below.</p>
<p><img alt="4e2e8d882b924d04aba6975d32887eb9" class="no-scaled-link" src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/rearrange-output.png" style="width: 950px;" /></p>
<p>When you actually need to calculate <code class="docutils literal notranslate"><span class="pre">a</span></code> (the output for a particular attention head), the easiest thing to do is just apply the appropriate slice of the linear map to <code class="docutils literal notranslate"><span class="pre">z</span></code> (we’ll get to this in the next exercise, so don’t worry about it for now).</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>Difficulty: 🔴🔴🔴🔴⚪
Importance: 🔵🔵🔵🔵🔵

You should spend up to 30-45 minutes on this exercise.
</pre></div>
</div>
<p>This is probably the most important function in today’s exercises. Implementing it will be pretty similar to the previous function <code class="docutils literal notranslate"><span class="pre">calculate_h_and_intervene</span></code>, but:</p>
<ul class="simple">
<li><p>Rather than extracting the value of the residual stream <code class="docutils literal notranslate"><span class="pre">h</span></code> at some particular layer, you’ll be extracting the output of the attention heads: iterating over each layer and each head in the model.</p>
<ul>
<li><p>You’ll only need to run one clean forward pass to compute all these values, but you’ll need to run a separate corrupted forward pass for each head.</p></li>
</ul>
</li>
<li><p>Rather than your 2 different datasets being (dataset, zero-shot dataset), your two datasets will be (dataset, corrupted version of that same dataset).</p>
<ul>
<li><p>You can use the <code class="docutils literal notranslate"><span class="pre">create_corrupted_dataset</span></code> method of the <code class="docutils literal notranslate"><span class="pre">AntonymDataset</span></code> class for this.</p></li>
</ul>
</li>
</ul>
<p><img alt="0a92fd8d34084cc0b5e8e47fdb092a2d" class="no-scaled-link" src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/cie-intervention.png" style="width: 1200px;" /></p>
<p>Before you actually start writing the code, it might be helpful to answer the following questions:</p>
<details><summary><p>What will your total batch size be (if your dataset has size N) ?</p>
</summary><p>Your batch size will be <code class="docutils literal notranslate"><span class="pre">N</span> <span class="pre">*</span> <span class="pre">((N_LAYERS</span> <span class="pre">*</span> <span class="pre">N_HEADS)</span> <span class="pre">+</span> <span class="pre">2)</span></code>, because you’ll need to run:</p>
<ul class="simple">
<li><p>A fowrad pass on the clean dataset (batch size <code class="docutils literal notranslate"><span class="pre">N</span></code>),</p></li>
<li><p>A forward pass on the corrupted dataset (batch size <code class="docutils literal notranslate"><span class="pre">N</span></code>) with no intevention,</p></li>
<li><p>A forward pass on the corrupted dataset (batch size <code class="docutils literal notranslate"><span class="pre">N</span></code>) once for each head, where you’re intervening on the heads.</p></li>
</ul>
</details><details><summary><p>Which proxy outputs (if any) will you need to use .save() on, in this function?</p>
</summary><p>None. You’re just doing causal interventions, and getting the logits. You don’t need to save the <span class="math notranslate nohighlight">\(z\)</span>-tensors in order to causally intervene with them at later points within the context manager (see the solution for the <code class="docutils literal notranslate"><span class="pre">calculate_h_and_intervene</span></code> exercises).</p>
</details><p>A few other tips:</p>
<ul class="simple">
<li><p>When it comes to intervening, you can set the value of a reshaped tensor, i.e. <code class="docutils literal notranslate"><span class="pre">tensor.reshape(*new_shape)[index]</span> <span class="pre">=</span> <span class="pre">new_value</span></code> will change the values in <code class="docutils literal notranslate"><span class="pre">tensor</span></code> without actually reshaping it (for more on this, see the documentation for <code class="docutils literal notranslate"><span class="pre">`torch.Tensor.view</span></code> &lt;<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.view.html">https://pytorch.org/docs/stable/generated/torch.Tensor.view.html</a>&gt;`__).</p></li>
<li><p>It’s good practice to insert a lot of assert statements in your code, to check the shapes are what you expect.</p></li>
<li><p>If you’re confused about dimensions, use <code class="docutils literal notranslate"><span class="pre">einops.rearrange</span></code> rather than <code class="docutils literal notranslate"><span class="pre">.reshape</span></code> - it’s like using code annotations within your actual code!</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calculate_fn_vectors_and_intervene</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">AntonymDataset</span><span class="p">,</span>
    <span class="n">LAYERS</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;layers heads&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Returns a tensor of shape (layers, heads), containing the CIE for each head.</span>

<span class="sd">    TODO - remove the LAYERS argument (it&#39;s only necessary because of batch size constraints, which will</span>
<span class="sd">    hopefully be fixed when servers work).</span>

<span class="sd">    Inputs:</span>
<span class="sd">        dataset: AntonymDataset</span>
<span class="sd">            the dataset of clean prompts from which we&#39;ll extract the function vector (we&#39;ll also create a</span>
<span class="sd">            corrupted version of this dataset for interventions)</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="n">LAYERS</span> <span class="o">=</span> <span class="n">LAYERS</span> <span class="k">if</span> <span class="p">(</span><span class="n">LAYERS</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="k">else</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span> <span class="c1"># range(N_LAYERS)</span>
    <span class="n">HEADS</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_HEADS</span><span class="p">)</span>

    <span class="c1"># Get corrupted dataset</span>
    <span class="n">corrupted_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_corrupted_dataset</span><span class="p">()</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="n">output_scores</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">generator</span><span class="p">:</span>

        <span class="c1"># Run a forward pass on clean prompts, where we store attention head outputs</span>
        <span class="n">a_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">prompts</span><span class="p">)</span> <span class="k">as</span> <span class="n">invoker</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">LAYERS</span><span class="p">:</span>
                <span class="c1"># Get hidden states, reshape to get head dimension, store the mean tensor</span>
                <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">input</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">a</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">N_HEADS</span><span class="p">,</span> <span class="n">D_HEAD</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="n">HEADS</span><span class="p">:</span>
                    <span class="n">a_dict</span><span class="p">[(</span><span class="n">layer</span><span class="p">,</span> <span class="n">head</span><span class="p">)]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">head</span><span class="p">]</span>

        <span class="c1"># Run a forward pass on corrupted prompts, where we don&#39;t intervene or store activations (just so we can</span>
        <span class="c1"># get the logits to compare with our intervention)</span>
        <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">corrupted_dataset</span><span class="o">.</span><span class="n">prompts</span><span class="p">)</span> <span class="k">as</span> <span class="n">invoker</span><span class="p">:</span>
            <span class="k">pass</span>

        <span class="c1"># For each head, run a forward pass on corrupted prompts (here we need multiple different forward passes,</span>
        <span class="c1"># because we&#39;re doing different interventions each time)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">LAYERS</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="n">HEADS</span><span class="p">:</span>
                <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">corrupted_dataset</span><span class="o">.</span><span class="n">prompts</span><span class="p">)</span> <span class="k">as</span> <span class="n">invoker</span><span class="p">:</span>
                    <span class="c1"># Get hidden states, reshape to get head dimension, then set it to the a-vector</span>
                    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">input</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                    <span class="n">hidden_states</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">N_HEADS</span><span class="p">,</span> <span class="n">D_HEAD</span><span class="p">)[:,</span> <span class="n">head</span><span class="p">]</span> <span class="o">=</span> <span class="n">a_dict</span><span class="p">[(</span><span class="n">layer</span><span class="p">,</span> <span class="n">head</span><span class="p">)]</span>


    <span class="c1"># Get output logits (which contains all `n_heads+2` sub-batches of size N, concatenated) and reshape into sub-batches</span>
    <span class="n">output_logits</span> <span class="o">=</span> <span class="n">einops</span><span class="o">.</span><span class="n">rearrange</span><span class="p">(</span><span class="n">generator</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">scores</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;(batch N) d_vocab -&gt; batch N d_vocab&quot;</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">output_logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">HEADS</span><span class="p">)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">LAYERS</span><span class="p">))</span> <span class="o">+</span> <span class="mi">2</span>

    <span class="c1"># Get the corrupted logits &amp; the logits with intervention (i.e. red in the diagram). Reshape latter to get head dim</span>
    <span class="n">logits_corrupted</span> <span class="o">=</span> <span class="n">output_logits</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">logits_intervention</span> <span class="o">=</span> <span class="n">einops</span><span class="o">.</span><span class="n">rearrange</span><span class="p">(</span><span class="n">output_logits</span><span class="p">[</span><span class="mi">2</span><span class="p">:],</span> <span class="s2">&quot;(layer head) N d_vocab -&gt; layer head N d_vocab&quot;</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="n">N_HEADS</span><span class="p">)</span>

    <span class="c1"># Get logprobs, for correct tokens</span>
    <span class="n">correct_completion_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">toks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">toks</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">completions</span><span class="p">)[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]]</span>
    <span class="n">logprobs_corrupted</span> <span class="o">=</span> <span class="n">logits_corrupted</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="n">correct_completion_ids</span><span class="p">]</span>
    <span class="n">logprobs_intervention</span> <span class="o">=</span> <span class="n">logits_intervention</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[:,</span> <span class="p">:,</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="n">correct_completion_ids</span><span class="p">]</span>

    <span class="c1"># Return mean effect of intervention, over the batch dimension</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">logprobs_intervention</span> <span class="o">-</span> <span class="n">logprobs_corrupted</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Need to run this in batches, for CUDA reasons (doing a forward pass with 5 prompts for each of the 28*16 heads in GPT-J is big!). I’ve attached the image, so I don’t have to run this again.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # 40 seconds for 2 layers, N=6, prepend=5</span>
<span class="c1"># # 85 seconds for 10 layers, N=6, prepend=5</span>
<span class="c1"># # (165 -&gt; CUDA error) seconds for 28 layers, N=6, prepend=5</span>

<span class="c1"># results = []</span>

<span class="c1"># dataset = AntonymDataset(WORD_PAIRS, size=6, n_prepended=3)</span>

<span class="c1"># for LAYERS in tqdm([range(10), range(10, 20), range(20, 28)]):</span>

<span class="c1">#     gc.collect()</span>
<span class="c1">#     t.cuda.empty_cache()</span>

<span class="c1">#     _results = calculate_fn_vectors_and_intervene(</span>
<span class="c1">#         dataset = dataset,</span>
<span class="c1">#         LAYERS = LAYERS,</span>
<span class="c1">#     )</span>
<span class="c1">#     results.append(_results)</span>

<span class="c1">#     gc.collect()</span>
<span class="c1">#     t.cuda.empty_cache()</span>


<span class="c1"># results = t.concat(results)</span>

<span class="c1"># imshow(</span>
<span class="c1">#     results.T,</span>
<span class="c1">#     title = &quot;Average indirect effect of function-vector intervention on antonym task&quot;,</span>
<span class="c1">#     width = 1000,</span>
<span class="c1">#     height = 600,</span>
<span class="c1">#     labels = {&quot;x&quot;: &quot;Layer&quot;, &quot;y&quot;: &quot;Head&quot;},</span>
<span class="c1">#     aspect = &quot;equal&quot;,</span>
<span class="c1"># )</span>
</pre></div>
</div>
</div>
<p><img alt="7b503ee6939a4570af5cf62405ebaa91" class="no-scaled-link" src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/main-result.png" style="width: 800px;" /></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>Difficulty: 🔴🔴🔴⚪⚪
Importance: 🔵🔵🔵⚪⚪

You should spend up to 20-35 minutes on this exercise.
</pre></div>
</div>
<p>Your next task is to actually calculate and return the function vector, so we can do a few experiments with it.</p>
<p>You should pick the 10 highest-scoring attention heads (according to the diagram above). You can hardcode these.</p>
<p>Mostly, this will involve taking your previous code and removing parts of it (since you only need to return the function vectors, not intervene with them). However, there is one difficulty here - in the previous exercises we causally intervened on the <code class="docutils literal notranslate"><span class="pre">out_proj</span></code> input (because that was easier), but here we need the actual attention head outputs. Can you see how to do this?</p>
<details><summary><p>Answer</p>
</summary><p>Once we have the value vectors post-attention (let’s call them <code class="docutils literal notranslate"><span class="pre">z</span></code>), we can just apply the linear map corresponding to a slice of the <code class="docutils literal notranslate"><span class="pre">out_proj</span></code> weight matrix.</p>
<p>To be more specific:</p>
<ul class="simple">
<li><p>The weight matrices of linear layers are stored as <code class="docutils literal notranslate"><span class="pre">(out_features,</span> <span class="pre">in_features)</span></code>, meaning <code class="docutils literal notranslate"><span class="pre">out_proj.weight.shape</span> <span class="pre">=</span> <span class="pre">(d_model,</span> <span class="pre">d_model</span> <span class="pre">=</span> <span class="pre">n_heads</span> <span class="pre">*</span> <span class="pre">d_head)</span></code> (i.e. the first dimension is the output space of the attention heads, and the second dimension is the input space for each of the <code class="docutils literal notranslate"><span class="pre">z</span></code>-vectors, concatenated).</p></li>
<li><p>We can rearrange this along the second dimension, then index into it to get a tensor of shape <code class="docutils literal notranslate"><span class="pre">(d_model,</span> <span class="pre">d_head)</span></code> corresponding to a particular head.</p></li>
<li><p>Then we can take this matrix <code class="docutils literal notranslate"><span class="pre">W_O</span></code>, and calculate <code class="docutils literal notranslate"><span class="pre">W_O</span> <span class="pre">&#64;</span> <span class="pre">z</span></code> - this will give us the attention head’s output.</p></li>
</ul>
</details><div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calculate_fn_vector</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">AntonymDataset</span><span class="p">,</span>
    <span class="n">head_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;d_model&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Returns a tensor of shape (layers, heads), containing the CIE for each head.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        dataset: AntonymDataset</span>
<span class="sd">            the dataset of clean prompts from which we&#39;ll extract the function vector (we&#39;ll also create a</span>
<span class="sd">            corrupted version of this dataset for interventions)</span>
<span class="sd">        head_list: List[Tuple[int, int]]</span>
<span class="sd">            list of attention heads we&#39;re calculating the function vector from</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># Turn head_list into a dict of {layer: list_of_heads}</span>
    <span class="n">head_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">head</span> <span class="ow">in</span> <span class="n">head_list</span><span class="p">:</span>
        <span class="n">head_dict</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">head_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="p">[])</span> <span class="o">+</span> <span class="p">[</span><span class="n">head</span><span class="p">]</span>

    <span class="n">z_dict</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="n">output_scores</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">generator</span><span class="p">:</span>

        <span class="c1"># output_list = []</span>

        <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">prompts</span><span class="p">)</span> <span class="k">as</span> <span class="n">invoker</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">head_list</span> <span class="ow">in</span> <span class="n">head_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>

                <span class="c1"># Get the output projection layer</span>
                <span class="n">out_proj</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">out_proj</span>

                <span class="c1"># Get the hidden states, and the mix of value vectors (which we&#39;ll call z)</span>
                <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">out_proj</span><span class="o">.</span><span class="n">input</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">z</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">),</span> <span class="n">N_HEADS</span><span class="p">,</span> <span class="n">D_HEAD</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

                <span class="c1"># For each head, compute the function vector, and add it to the list</span>
                <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="n">head_list</span><span class="p">:</span>
                    <span class="n">z_dict</span><span class="p">[(</span><span class="n">layer</span><span class="p">,</span> <span class="n">head</span><span class="p">)]</span> <span class="o">=</span> <span class="n">z</span><span class="p">[</span><span class="n">head</span><span class="p">]</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

        <span class="c1"># fn_vector = t.stack(output_list).sum(dim=0).save()</span>

    <span class="n">attn_head_output_dict</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">head</span><span class="p">),</span> <span class="n">z</span> <span class="ow">in</span> <span class="n">z_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">W_O</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">local_model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">W_O</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">D_MODEL</span><span class="p">,</span> <span class="n">N_HEADS</span><span class="p">,</span> <span class="n">D_HEAD</span><span class="p">)[:,</span> <span class="n">head</span><span class="p">]</span> <span class="o">@</span> <span class="n">z</span><span class="o">.</span><span class="n">value</span>
        <span class="n">attn_head_output_dict</span><span class="p">[(</span><span class="n">layer</span><span class="p">,</span> <span class="n">head</span><span class="p">)]</span> <span class="o">=</span> <span class="n">output</span>

    <span class="n">fn_vector</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">attn_head_output_dict</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">fn_vector</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">D_MODEL</span><span class="p">,)</span>

    <span class="k">return</span> <span class="n">fn_vector</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tests</span><span class="o">.</span><span class="n">test_calculate_fn_vector</span><span class="p">(</span><span class="n">calculate_fn_vector</span><span class="p">)</span>
</pre></div>
</div>
</div>
<section id="Multi-token-generation">
<h2>Multi-token generation<a class="headerlink" href="#Multi-token-generation" title="Permalink to this heading">#</a></h2>
<p>We’re now going to replicate some of the results in Table 3, in the paper:</p>
<p><img alt="d57260731e0a490a8f22c1090681ca53" class="no-scaled-link" src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/tab3.png" style="width: 700px;" /></p>
<p>This will involve doing something we haven’t done before - <strong>intervening on multi-token prompt generation</strong>.</p>
<p>Most of the interpretability exercises in this chapter have just consisted of running single forward passes, rather than autoregressive text generation. But we’re trying something different here: we’re adding the function vector to the final sequence position at each forward pass during text generation, and seeing if we can get the model to output a sentence with a different meaning.</p>
<p>The results of Table 3 came from adding the function vector to the residual stream at the final sequence position of the original prompt, <strong>and the final sequence position for each subsequent generation.</strong> The reason we do this is to guide the model’s behaviour over time. Our hypothesis is that the function vector induces “next-token antonym behaviour” (because it was calculated by averaging attention head outputs at the sequence position before the model made its antonym prediction in the ICL
prompts).</p>
<section id="Using-nnsight-for-multi-token-generation">
<h3>Using <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> for multi-token generation<a class="headerlink" href="#Using-nnsight-for-multi-token-generation" title="Permalink to this heading">#</a></h3>
<p>Previously, our context managers have looked like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">as</span> <span class="n">generator</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="k">as</span> <span class="n">invoker</span><span class="p">:</span>

        <span class="c1"># Do stuff to the model&#39;s internals</span>
</pre></div>
</div>
<p>But for multi-token generation, our context mnagers will look like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="o">=</span><span class="n">max_new_tokens</span><span class="p">)</span> <span class="k">as</span> <span class="n">generator</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="k">as</span> <span class="n">invoker</span><span class="p">:</span>

        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>
            <span class="c1"># Do stuff to the model&#39;s internals, on the n-th forward pass</span>
            <span class="n">invoker</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
</pre></div>
</div>
<p>The line <code class="docutils literal notranslate"><span class="pre">invoker.next()</span></code> denotes that the following interventions should be applied to the subsequent generations.</p>
<p>Mostly, the stuff you’ll be used to from single-token generation generalizes to th multi-token case. The object <code class="docutils literal notranslate"><span class="pre">generator.output</span></code> is still a tensor which contains the model’s token ID completions (or we can return scores instead, exactly like we did before). Using <code class="docutils literal notranslate"><span class="pre">.save()</span></code> still saves proxies outside the context managers (although make sure that you don’t use the same variable names over different generations, otherwise you’ll overwrite them - it’s easier to store your saved proxies in
e.g. a list or dict).</p>
<p>A couple more notes:</p>
<ul class="simple">
<li><p>By default the <code class="docutils literal notranslate"><span class="pre">generate</span></code> method will generate tokens greedily, i.e. always taking the maximum-probability token at each step. For now, we don’t need to worry about changing this behaviour.</p></li>
<li><p>Transformer models perform <strong>key-value caching</strong> to speed up text generation. This means that the time taken to generate <span class="math notranslate nohighlight">\(n\)</span> tokens is <strong>much</strong> less than <span class="math notranslate nohighlight">\(n\)</span> times longer than generating a single token. See <a class="reference external" href="https://kipp.ly/transformer-inference-arithmetic/">this blog post</a> on transformer inference arithmetic for more.</p></li>
</ul>
</section>
<section id="Exercise---intervene-with-function-vector,-in-multi-token-generation">
<h3>Exercise - intervene with function vector, in multi-token generation<a class="headerlink" href="#Exercise---intervene-with-function-vector,-in-multi-token-generation" title="Permalink to this heading">#</a></h3>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>Difficulty: 🔴🔴🔴⚪⚪
Importance: 🔵🔵🔵🔵⚪

You should spend up to 10-20 minutes on this exercise.
</pre></div>
</div>
<p>You should now fill in the function <code class="docutils literal notranslate"><span class="pre">intervene_with_fn_vector</span></code> below. This will take a function vector (calculated from the function you wrote above), as well as a few other arguments (see docstring), and return the model’s string completion on the given prompt template.</p>
<p>We hope to observe results qualitatively like the ones in Table 3, i.e. having the model define a particular word as its antonym.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">intervene_with_fn_vector</span><span class="p">(</span>
    <span class="n">word</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">layer</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">fn_vector</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;d_model&quot;</span><span class="p">],</span>
    <span class="n">prompt_template</span> <span class="o">=</span> <span class="s1">&#39;The word &quot;</span><span class="si">{x}</span><span class="s1">&quot; means&#39;</span><span class="p">,</span>
    <span class="n">n_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Intervenes with a function vector, by adding it at the last sequence position of a generated prompt.</span>

<span class="sd">    Returns: the full completion (including original prompt) for no-intervention / intervention case respectively.</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="n">prompt</span> <span class="o">=</span> <span class="n">prompt_template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">word</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="o">=</span><span class="n">n_tokens</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">)</span> <span class="k">as</span> <span class="n">generator</span><span class="p">:</span>

        <span class="c1"># No intervention</span>
        <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="k">as</span> <span class="n">invoker</span><span class="p">:</span>
            <span class="k">pass</span>

        <span class="c1"># Intervention</span>
        <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="k">as</span> <span class="n">invoker</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_tokens</span><span class="p">):</span>
                <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">hidden_states</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">fn_vector</span>
                <span class="c1"># TODO - add exercises about invoker.next()!</span>
                <span class="n">invoker</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">generator</span><span class="o">.</span><span class="n">output</span>

    <span class="n">completions</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">completions</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>To test your function, run this code. You should find that the first completion seems normal, but the second completion defines a word as its antonym. <strong>You’ve just successfully induced an OOD behavioural change in a 6b-parameter model!</strong></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define our dataset, and the attention heads we&#39;ll use</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">AntonymDataset</span><span class="p">(</span><span class="n">WORD_PAIRS</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_prepended</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">head_list</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">14</span><span class="p">),</span> <span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">12</span><span class="p">),</span> <span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">13</span><span class="p">),</span> <span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span> <span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">14</span><span class="p">)]</span>

<span class="c1"># Extract the function vector</span>
<span class="n">fn_vector</span> <span class="o">=</span> <span class="n">calculate_fn_vector</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">head_list</span><span class="p">)</span>

<span class="c1"># Define a word we&#39;ll use in our prompt, and check it&#39;s OOD</span>
<span class="n">word</span> <span class="o">=</span> <span class="s2">&quot;expensive&quot;</span>
<span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">pair</span> <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">WORD_PAIRS</span><span class="p">)</span>

<span class="c1"># Intervene with the function vector</span>
<span class="n">completion</span><span class="p">,</span> <span class="n">completion_intervention</span> <span class="o">=</span> <span class="n">intervene_with_fn_vector</span><span class="p">(</span>
    <span class="n">word</span> <span class="o">=</span> <span class="n">word</span><span class="p">,</span>
    <span class="n">layer</span> <span class="o">=</span> <span class="mi">9</span><span class="p">,</span>
    <span class="n">fn_vector</span> <span class="o">=</span> <span class="n">fn_vector</span><span class="p">,</span>
    <span class="n">prompt_template</span> <span class="o">=</span> <span class="s1">&#39;The word &quot;</span><span class="si">{x}</span><span class="s1">&quot; means&#39;</span><span class="p">,</span>
    <span class="n">n_tokens</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">completion</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">completion_intervention</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&#39;The word &#34;expensive&#34; means different things to different people. For some, it&#39;
&#39;The word &#34;expensive&#34; means &#34;cheap&#34; in this case.\n\n&#39;
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Note to self - won&#39;t have ppl replicate results like Table 2, because this just seems kinda boring compared to the stuff we&#39;ve done so far, and it doesn&#39;t introduce any new conceptual ideas. We&#39;ve already done (zero shot, h-vector) and (shuffled, fn-vector). We&#39;ll leave as a bonus exercise replicating these results with other prompt templates and other models.</span>

<span class="c1"># TODO - change the examples in the `generate_dataset` function, so they have a prepended space</span>
</pre></div>
</div>
</div>
</section>
<section id="Exercise---generalize-results-to-another-task-(optional)">
<h3>Exercise - generalize results to another task (optional)<a class="headerlink" href="#Exercise---generalize-results-to-another-task-(optional)" title="Permalink to this heading">#</a></h3>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>Difficulty: 🔴🔴🔴🔴⚪
Importance: 🔵🔵🔵⚪⚪

You should spend up to 10-15 minutes on this exercise.
</pre></div>
</div>
<p>In this exercise, you get to pick a task different to the antonyms task, and see if the results still hold up (for the same set of attention heads).</p>
<p>We’ll leave this exercise fairly open-ended, without any code templates for you to fill in. However, if you’d like some guidance you can use the dropdown below.</p>
<details><summary><p>Guidance for exercise</p>
</summary><p>Whatever your task, you’ll want to generate a new set of words. You can repurpose the <code class="docutils literal notranslate"><span class="pre">generate_dataset</span></code> function from the antonyms task, by supplying a different prompt and initial set of examples (this will require generating &amp; using an OpenAI api key, if you haven’t already), or you can just find an appropriate dataset online.</p>
<p>When you define the <code class="docutils literal notranslate"><span class="pre">AntonymDataset</span></code>, you might want to use <code class="docutils literal notranslate"><span class="pre">bidirectional=False</span></code>, if your task isn’t symmetric. The antonym task is symmetric, but others (e.g. the Country-Capitals task) are not.</p>
<p>You’ll need to supply a new prompt template for the <code class="docutils literal notranslate"><span class="pre">intervene_with_fn_vector</span></code> function, but otherwise most of your code should stay the same.</p>
</details><div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">generate_dataset_capitals</span><span class="p">(</span><span class="n">N</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>

    <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="n">examples</span> <span class="o">=</span> <span class="s2">&quot;Portugal: Lisbon, Ireland: Dublin, Chile: Santiago, Japan: Tokyo, &quot;</span>

    <span class="n">response</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">ChatCompletion</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4&quot;</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Give me </span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s2"> examples of Country-Capital pairs.&quot;</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Sure! Here are </span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s2"> Country-Capital pairs: </span><span class="si">{</span><span class="n">examples</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">},</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="n">response_text</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">examples</span> <span class="o">+</span> <span class="n">response</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;message&quot;</span><span class="p">][</span><span class="s2">&quot;content&quot;</span><span class="p">]</span>

    <span class="n">word_pairs</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_pair</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;: &quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">word_pair</span> <span class="ow">in</span> <span class="n">response_text</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s2">&quot;.</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;, &quot;</span><span class="p">)]</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Finished in </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> seconds.&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">word_pairs</span>


<span class="n">CC_WORD_PAIRS</span> <span class="o">=</span> <span class="n">generate_dataset_capitals</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Finished in 35.82 seconds.
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Remove (Netherlands, Amsterdam) from the pairs, so it can be a holdout</span>
<span class="n">country</span> <span class="o">=</span> <span class="s2">&quot;Netherlands&quot;</span>
<span class="n">_CC_WORD_PAIRS</span> <span class="o">=</span> <span class="p">[</span><span class="n">pair</span> <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">CC_WORD_PAIRS</span> <span class="k">if</span> <span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">country</span><span class="p">]</span>

<span class="c1"># Define our dataset, and the attention heads we&#39;ll use</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">AntonymDataset</span><span class="p">(</span><span class="n">_CC_WORD_PAIRS</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_prepended</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">head_list</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">14</span><span class="p">),</span> <span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">12</span><span class="p">),</span> <span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">13</span><span class="p">),</span> <span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span> <span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">14</span><span class="p">)]</span>

<span class="c1"># Extract the function vector</span>
<span class="n">fn_vector</span> <span class="o">=</span> <span class="n">calculate_fn_vector</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">head_list</span><span class="p">)</span>

<span class="c1"># Intervene with the function vector</span>
<span class="n">completion</span><span class="p">,</span> <span class="n">completion_intervention</span> <span class="o">=</span> <span class="n">intervene_with_fn_vector</span><span class="p">(</span>
    <span class="n">word</span> <span class="o">=</span> <span class="n">country</span><span class="p">,</span>
    <span class="n">layer</span> <span class="o">=</span> <span class="mi">9</span><span class="p">,</span>
    <span class="n">fn_vector</span> <span class="o">=</span> <span class="n">fn_vector</span><span class="p">,</span>
    <span class="n">prompt_template</span> <span class="o">=</span> <span class="s1">&#39;When you think of </span><span class="si">{x}</span><span class="s1">,&#39;</span><span class="p">,</span>
    <span class="n">n_tokens</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">completion</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">completion_intervention</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&#39;When you think of Netherlands, you probably think of tulips, windmills, and cheese. But the Netherlands is also home&#39;
&#39;When you think of Netherlands, you probably think of tulips, windmills, and cheese. But Amsterdam is a lot more&#39;
</pre></div></div>
</div>
</section>
</section>
</section>
<section id="id4">
<h1>4️⃣ Bonus<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h1>
<p>There are two other interesting results from the paper, although neither of them are as important as the ones we’ve covered so far. If you have time, you can try to reproduce these results yourself.</p>
<p>In this section, the authors find the top words in the decoded vocabulary of the function vector (i.e. the words whose unembedding vectors have the highest dot product with the function vector), and show that these words seem conceptually related to the task. For example:</p>
<ul class="simple">
<li><p>For the antonyms task, the top words evoke the idea of antonyms, e.g. <code class="docutils literal notranslate"><span class="pre">&quot;</span> <span class="pre">negate&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;</span> <span class="pre">counterpart&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;</span> <span class="pre">lesser&quot;</span></code>.</p></li>
<li><p>For the country-capitals task, the top words are actually the names of capitals, e.g. <code class="docutils literal notranslate"><span class="pre">&quot;</span> <span class="pre">Moscow&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;</span> <span class="pre">Paris&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;</span> <span class="pre">Madrid&quot;</span></code>.</p></li>
</ul>
<p>Can you replicate these results, both with the antonyms task and with the task you chose in the previous section?</p>
<p>An interesting extension - what happens if you take a task like the Country-Capitals task (which is inherently asymmetric), and get your function vector from the symmetric version of the task (i.e. the one where each of your question-answer pairs might be flipped around)? Do you still get the same behavioural results, and how (if at all) do the decoded vocabulary results change?</p>
<details><summary><p>My results for this (spoiler!)</p>
</summary><p>In the Country-Capitals task, I found:</p>
<ul class="simple">
<li><p>The bidirectional task does still work to induce behavioural changes, although slightly less effectively than for the original task.</p></li>
<li><p>The top decoded vocabulary items are a mix of country names and capital names, but mostly capitals.</p></li>
</ul>
</details><p>In this section, the authors investigate whether function vectors can be composed. For instance, if we have three separate ICL tasks which in some sense compose to make a fourth task, can we add together the three function vectors of the first tasks, and use this as the function vector of the fourth task?</p>
<p>The authors test this on a variety of different tasks. They find that it’s effective on some tasks (e.g. Country-Capitals, where it outperforms function vectors), but generally isn’t as effective as function vectors. Do you get these same results?</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Code to calculate decoded vocabulary:</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">local_model</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">fn_vector</span><span class="p">)</span>
<span class="n">max_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">indices</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">max_logits</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>


                </article>
              
              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="main_demo.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Main Demo</p>
      </div>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">NDIF Main Demo Notebook</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Content-&amp;-Learning-Objectives">Content &amp; Learning Objectives</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#1️⃣-Introduction-to-nnsight">1️⃣ Introduction to <code class="docutils literal notranslate"><span class="pre">nnsight</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#2️⃣-Task-encoding-hidden-states">2️⃣ Task-encoding hidden states</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#3️⃣-Function-Vectors">3️⃣ Function Vectors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#4️⃣-Bonus">4️⃣ Bonus</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#Setup">Setup</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">1️⃣ Introduction to <code class="docutils literal notranslate"><span class="pre">nnsight</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Important-syntax">Important syntax</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Model-config">Model config</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Tokenizers">Tokenizers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Model-outputs">Model outputs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Running-the-model">Running the model</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#model.transformer.h[-1]"><code class="docutils literal notranslate"><span class="pre">model.transformer.h[-1]</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#.output[0]"><code class="docutils literal notranslate"><span class="pre">.output[0]</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#.save()"><code class="docutils literal notranslate"><span class="pre">.save()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Exercise---visualize-attention-heads">Exercise - visualize attention heads</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">2️⃣ Task-encoding hidden states</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Antonym-Dataset">Antonym Dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Exercise---run-forward-pass-on-antonym-dataset">Exercise - run forward pass on antonym dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Exercise---intervene-with-h">Exercise - intervene with <span class="math notranslate nohighlight">\(h\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Exercise---combine-the-functions-calculate_h-and-intervene_with_h">Exercise - combine the functions <code class="docutils literal notranslate"><span class="pre">calculate_h</span></code> and <code class="docutils literal notranslate"><span class="pre">intervene_with_h</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Logit-outputs">Logit outputs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Exercise---compute-change-in-accuracy">Exercise - compute change in accuracy</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">3️⃣ Function Vectors</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Multi-token-generation">Multi-token generation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Using-nnsight-for-multi-token-generation">Using <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> for multi-token generation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Exercise---intervene-with-function-vector,-in-multi-token-generation">Exercise - intervene with function vector, in multi-token generation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Exercise---generalize-results-to-another-task-(optional)">Exercise - generalize results to another task (optional)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">4️⃣ Bonus</a></li>
</ul>

  </nav></div>

  <div class="sidebar-secondary-item">
  <div class="tocsection sourcelink">
    <a href="../_sources/tutorials/nnsight_exercises.ipynb.txt">
      <i class="fa-solid fa-file-lines"></i> Show Source
    </a>
  </div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
  <p class="copyright">
    
      © Copyright 2023, Jaden Fiotto-Kaufman.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">
  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 5.3.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.13.3.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>